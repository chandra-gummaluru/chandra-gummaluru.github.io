---
layout: post
title: "Course Notes: Convex Optimization"
author: "Chandra Gummaluru"
---

The following notes are heavily based on material developed by Professor Stephen Boyd for [EE364 taught at Stanford University](http://web.stanford.edu/class/ee364a/lectures.html). All typos are my own.

# Optimization Problems
Our goal is to solve problems of the following form:
\\[\begin{equation}\begin{aligned}\text{minimize } &f(x) \\\\ \text{subject to } &g_i(x) \leq 0, i = 1,\dots, m \\\\ &h_j(x) =0, j = 1, \dots, n\end{aligned}\end{equation}\tag{OptPrb}\\]
where
 - $x \in \mathbb{R}^n$ is called the **optimization variable**
 - $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is called the **objective function**
 - $g_i: \mathbb{R}^n \rightarrow \mathbb{R}, i =1, \dots, m$ are called the **inequality constraints**, and
 - $h_j: \mathbb{R}^n \rightarrow \mathbb{R}, j =1, \dots n$ are called the **equality constraints**.

The $g_i$ and $h_j$ are the **explicit constraints**. However, there is also the **implicit constraint** that $x$ must be within intersection of the domains of $f, g_1 \dots, g_m, h_1, \dots, h_n$, i.e.,
\\[\mathcal{D} := \text{dom}(f) \cap \bigcap_{i=1}^{m}\text{dom}(g_i) \cap \bigcap_{j=1}^{n}\text{dom}(h_i).\\]

> **Def. Feasible Point**:
> A point $x \in \mathcal{D}$, is **feasible** for (OptPrb) if it also satisfies its explicit constraints, i.e., iff
> \\[g_i(x) \leq 0, i = 1, \dots, m,\\]
> and
> \\[h_j(x) = 0, j = 1, \dots, n.\\]

We will use $\mathcal{X}$ to denote the set of feasible points. Note that $\mathcal{X} \subseteq \mathcal{D}$.

> **Def. Feasible Direction**:
> Given a feasible point, $x \in \mathcal{X}$ for (OptPrb), a direction, $v \in \mathbb{R}^n, \\|v\\|\_2 = 1$ is **feasible** if there exists an $t > 0$ for which $x + tv \in \mathcal{X}$.

We will use $\mathcal{V}(x)$ to denote the set of feasible directions from a feasible point, $x$.

> **Def. Globally-Optimal Point**:
> A feasible point, $x^* \in \mathcal{X}$, is **globally optimal** for (OptPrb) if
> \\[f(x^*) = \underset{x \in \mathcal{X}}{\text{inf}}\big\lbrace f(x) \big\rbrace.\\]

> **Def. Locally-Optimal Point**:
> A feasible point, $x$, is **locally optimal** for (OptPrb) if there exists an $R > 0$, such that $x$ is globally optimal for the optimization problem:
> \\[\begin{aligned}\text{minimize } &f(z) \\\\ \text{subject to } &g_i(z) \leq 0, i = 1,\dots, m \\\\ &h_j(z) =0, j = 1, \dots, n \\\\ & \\|x- z\\|_2 \leq R \end{aligned}.\\]

A locally optimal point need not be globally optimal in general.

## Definitions and Theorems from Vector Calculus
Looking at (OptPrb), we need to optimize (minimize) a multi-variable scalar function, i.e., a function of the form, $f: \mathbb{R}^n \rightarrow \mathbb{R}$, where $x \mapsto f(x)$. We will use the notation, $\text{dom}(f)$ and $\text{cdm}(f)$ to respectively denote the domain and co-domains of $f$. Note that $\text{dom}(f) \subseteq \mathbb{R}^n$ and $\text{cdm}(f) \subseteq \mathbb{R}$.

> **Def. Derivative**:
> Let $v \in \mathbb{R}^n, \|v\|\_2 = 1$. The $v$-**directional derivative** of a function, $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is a function, denoted $D_{v}f: \mathbb{R}^n \rightarrow \mathbb{R}$, where $D_vf(z)$ gives the rate of change of $f$ at $z$ in the direction $v$, and is defined point-wise as
> \\[D_{v}f(z) := \lim_{t \rightarrow 0}\frac{f(z + tv) - f(z)}{t}.\\]
> If $v = e^{(i)}$, we call $D_{v}f$ the **partial derivative** of $f$ w.r.t. $x_i$, and denote it as $\partial f / \partial x_i$.
 
Using the aformentioned definition to compute derivatives is difficult. However, there are a few theorems that can help:
 
> **Thm. Chain Rule**:
> Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$, $g: \mathbb{R} \rightarrow \mathbb{R}^n$ and define $h := f \circ g$. Then,
> \\[\frac{dh}{dt} = \sum_{i=1}^{n}\frac{\partial h}{\partial g_i}\frac{dg_i}{dt}.\\]

It turns out that we can compute the directional derivative of $f$ is any direction, $v$, using just its partial derivatives.
> **Thm. Computing Directional Derivatives from Partial Derivatives**:
> If $f: \mathbb{R}^n \rightarrow \mathbb{R}$, then for any $z$,
> \\[D_vf(z) = \nabla f^{\top}(z)v\\]
> where
> \\[\nabla f(z)\_i := \left.\frac{\partial f}{\partial x_i}\right\vert_{x=z}.\\]

<details>
 <summary><strong>See proof</strong>.</summary>
<p>
Define the function $g(t) = z + tv$. Then,
\[D_vf(z) = \lim_{t \rightarrow 0}\frac{f(z+tv) - f(z)}{t}\]
$\begin{aligned}
&= \lim_{t\rightarrow 0}\frac{f\left(g(t)\right) - f(\left(g(0)\right)}{t} \\
&= \lim_{t\rightarrow 0}\frac{h(t) - h(0)}{t} \\
&= \left.\frac{dh}{dt}\right\vert_{t=0}
\end{aligned}$
<br><br>
Using the chain-rule, we have
\[\left.\frac{\partial h}{\partial t}\right\vert_{t=0} = \sum_{i=1}^{n}\left.\frac{\partial h}{\partial g_i}\right\vert_{t=0}\left.\frac{dg_i}{dt}\right\vert_{t=0}\]
$\begin{aligned}
&= \sum_{i=1}^{n}\left.\frac{\partial h}{\partial g_i}\right\vert_{t=0}v_i \\
&= \sum_{i=1}^{n}\left.\frac{\partial f}{\partial x_i}\right\vert_{x=z}v_i \\
&= \nabla f^{\top}(z)v
\end{aligned}$
<br>
$\blacksquare$
</p>
</details>
<br>
We call $\nabla f(z)$ the **gradient** of $f$ at $z$. We say $f$ is **differentiable** at $z$ if $\nabla f(z)$ exists.

A vector function, $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ can be interpreted as $m$ scalar functions, $f_1, \dots, f_m: \mathbb{R}^n \rightarrow \mathbb{R}$. Thus, we define its directional derivatives and gradient accordingly, i.e.,
\\[D_vf = \begin{bmatrix} D_vf_1 \\\\ \vdots \\\\ D_vf_m \end{bmatrix} \text{ and } \nabla f^{\top} = \begin{bmatrix} \nabla f^{\top}_1 \\\\ \vdots \\\\ \nabla f^{\top}_m \end{bmatrix}.\\]

We can use the aforementioned definitions to compute higher-order derivatives of a scalar function, $f: \mathbb{R}^n \rightarrow \mathbb{R}$. For example, the 2<sup>nd</sup> derivative of $f$ is the gradient of the gradient of $f$, i.e.,
\\[\nabla^2 f = \nabla\left(\nabla f\right) = \begin{bmatrix} \frac{\partial^2 f}{\partial x_1^2} & \dots & \frac{\partial^2 f}{\partial x_1x_n} \\\\ \vdots & \ddots & \vdots \\\\ \frac{\partial^2 f}{\partial x_nx_1} & \dots & \frac{\partial^2 f}{\partial x_n^2}\end{bmatrix}.\\]
> **Thm. Computing Higher-Order Directional Derivatives from Partial Derivatives:**
> Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$. Then, $D_{u}\left(D_vf\right) = u^{\top}\nabla ^2 fv$, i.e., $u^{\top}\nabla ^2fv$ is the $u$-directional derivative of the $v$-directional derivative of $f$.
> 
<details>
 <summary><strong>See proof</strong>.</summary>
<p>
We have
\[u^{\top}\nabla^2 f(z)v = \sum_{i=1}^{n}\sum_{j=1}^{n}u_iv_j\frac{\partial^2 f}{\partial x_i \partial x_j}\]
$\begin{aligned}
&= \sum_{i=1}^{n}\sum_{j=1}^{n}u_iv_j\frac{\partial^2 f}{\partial x_i \partial x_j} \\
&= \sum_{i=1}^{n}u_i\frac{\partial}{\partial x_i}\left(\sum_{j=1}^{n}v_j\frac{\partial f}{\partial x_j}\right) \\
&= \sum_{i=1}^{n}u_i\frac{\partial}{\partial x_i}\left(\nabla f^{\top}v\right) \\
&= \nabla \left(\nabla f^{\top}v\right)^{\top}u \\
&= D_{u}\left(D_{v}f\right).
\end{aligned}$
<br>
$\blacksquare$
</p>
</details>
<br>
We call $\nabla^2f(z)$ the **Hessian** of $f$ at $z$. We say $f$ is **twice-differntiable** at $z$ if $\nabla^2f(z)$ exists.

## Conditions for Optimality
We now use the aformentioned defintions to derive a few conditons under which a point, $x^*$ is locally optimal.
> **Thm. 1<sup>st</sup>-Order Necessary Condition for Optimality**:
> If $x^\*$ is a local minimizer of (OptPrb), then,
> \\[\nabla f^{\top}(x^\*)v \geq 0, \forall v \in \mathcal{V}(x^\*).\\]

<details>
 <summary><strong>See proof</strong>.</summary>
<p>
Since $x^\*$ is a local minimizer, it follows that $\exists R > 0$, such that if $\|z - x^\*\|_2 \leq R$, then $f(z) > f(x^\*)$. Without loss of generality, we may express $z = x^\* + tv$, where $v \in \mathcal{V}(x^\*)$. Thus, the condition, $\|z-x^*\|_2 \leq R$, can be written as $\|tv\|_2 \leq R$, or simply $t \leq R$, since $\|v\|_2 = 1$.
<br>
Thus, for all $t \leq R$, we have $f\left(x^\*+tv\right) \geq f(x^\*)$, which means that
\[D_vf\left(x^\*\right) := \lim_{t\rightarrow 0}\frac{f\left(x^\*+tv\right) - f(x^\*)}{t} = \nabla f^{\top}(x^\*)v \geq 0.\]
$\blacksquare$
</p>
</details>
## Finding a Lower-Bound on $f(x^*)$
Finding a globally-optimal or even locally-optimal point, $x^*$ for (OptPrb) is difficult. However, we can derive a lower-bound for $f(x^\*)$. To do this, we first define two functions,
\\[L_f(x,\lambda,\mu) = f(x) + \sum_{i=1}^{m}\lambda_ig_i(x) + \sum_{j=1}^{n}\nu_jh_j(x),\\]
and
\\[\zeta_f(\lambda,\nu) = \underset{x \in \mathcal{X}}{\text{inf}}L_f(x,\lambda,\nu)\\]
where $\lambda_i \geq 0, i = 1, \dots, m$. These definitions may seem arbitrary, but the idea is that for any feasible $x$, $L_f(x,\lambda,\mu) \leq f(x)$, and so $\zeta_f(\lambda,\nu) \leq f(x^\*)$.
<br><br>   
Obviously, we want to choose $\lambda$ and $\nu$ so that $\zeta_f(\lambda,\nu)$ is as large as possible. This can be done by solving the maximization problem:
\\[\begin{aligned}\text{maximize } &\zeta_f(\lambda,\mu) \\\\ \text{subject to } &\lambda_i \geq 0, i = 1,\dots, m \end{aligned}\tag{DualOptPrb}\\]
We refer to (OptDualPrb) as the **dual problem** of (OptPrb). This may not seem very helpful, since we have just replaced one optimization problem with another. However, as we will see later, (OptDualPrb) is often much easier to solve.

# Convex Optimization Problems
Our only hope of solving (OptPrb) is to make some assumptions on $f$, $g_1,\dots,g_m$ and $h_1,\dots,h_n$. To state these assumptions, we will require a few definitions.

## Convex Sets 
> **Def. Convex Set**:
> A **convex combination** of the points, $x_1, \dots, x_n$ is of the form
> \\[t_1x_1 + \dots t_nx_n\\] where $t_1,\dots,t_n \geq 0$, and $t_1 + \dots +t_n = 1$.
> A set of points, $S$, is **convex** if any convex combination of any subset of its points is within $S$.

### Examples of Convex Sets
We now consider some examples of convex sets:
> **Def. Hyperplane:**
> A **hyperplane**, is a set of the form, $\left\lbrace a^{\top}x = b \right\rbrace$, where $a \neq 0$. Hyper-planes are convex.

> **Def. Half-Space**:
> A **halfspace**, is a set of the form, $\left\lbrace a^{\top}x \leq b \right\rbrace$, where $a \neq 0$. Half-spaces are convex.

> **Def. Norm-Ball**:
> A **norm ball** of radius, $r$, with center $x_c$, is a set of the form
> \\[\left\lbrace x \text{ s.t. } \|x - x_c\| \leq r\right\rbrace.\\]
> Norm balls are convex.

> **Def. Norm-Cone**:
> **A **norm cone** is a set of the form
> \\[\left\lbrace x \text{ s.t. } \|x\| \leq t \right\rbrace.\\]
> Norm cones are convex.

> **Def. Polyhedron**:
> A **polyhedron** is the solution set of finitely many linear equalities and/or inequalities:
> \\[Ax \preceq b \text{ and } Cx = d \\] In other words, it is a set of the form
> \\[\left\lbrace x \text{ s.t. } Ax \preceq b, Cx = d\right\rbrace.\\]
> Polyhedra are convex.

### Operations that Preserve Convexity
To show that a set, $S$, is convex, one can resort to the definition, i.e., show that
\\[x_1, x_2 \in S, t \in [0, 1] \Rightarrow tx_1 +(1-t)x_2 \in S\\]However, this is often difficult to do in practice.

Another option is to show that $S$ can be formed from convex sets using operations that preserve convexity. Such operations include, but are not limited to:

 - **Intersections:** If $S_1$ and $S_2$ are convex, then so is $S_1 \cap S_2$.
 
 - **Affine Correspondences:** If $S$ is a convex set, and $f(x) = Ax+ b$, then
   - the image of $S$ under $f$, i.e., $f(S) = \left\lbrace f(x) \text{ s.t. } x \in S \right\rbrace$ is also convex
   - the inverse image of $S$ under $f$, i.e., $f^{-1}(S) = \left\lbrace x \text{ s. t. } f(x) \in S \right\rbrace$ is also convex.

## Convex Functions
> **Def. Convex Function**<br>
> A function, $f: \mathbb{R}^n \rightarrow \mathbb{R}$, is **convex**, iff $\text{dom}(f)$ is a convex set and
> \\[f(tx + (1-t)y) \leq tf(x)+(1-t)f(y)\\]for all $x, y \in \text{dom}(f)$ and $t \in [0,1]$.
> We say that $f$ is strictly convex if it is convex and
> \\[f(tx+(1-t)y) < tf(x) +(1-t)y\\]
> for $t \in (0,1)$.


### Necessary and Sufficient Conditions for Convexity
To check whether a function, $f$, is covnex or not, one can use the aformentioned definition. However, depending on $f$ this may be very tedious. Thus, it is useful to derive some other conditions that are both necessary and sufficient for $f$ to be convex.

> **Thm. Covnexity under Linear Restrictions** <br>
> Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ and define $g(t) := f(x_0 + tv)$. Then, $f$ is convex (in $x$) if and only if $g$ is convex (in $t$) for every $x_0 \in \text{dom}(f)$ and $v \in \mathbb{R}^n$.

We will make use of the aformentioned condition to derive two additional conditions.
> **Thm. First-Order Characterization of Convexity**<br>
> If $f$ is differentiable, then it is convex iff
> $f(y) \geq f(x) + \nabla f(x)^{\top}(y-x).$

<details>
 <summary><strong>See proof</strong>.</summary>
<p>
Let $g(t) = f(x_0 + tv)$. Then $f$ is convex if and only if $g$ is convex for all $v \in \mathbb{R}$, $x_0 \in \textrm{Dom}(f)$, where we restrict the domain of $t$ so that $x_0 + tv \in \textrm{Dom}(f)$.<br><br>

$\Rightarrow$: Since $g$ is a scalar function of $t$, if it is convex, then

\[\underbrace{g(t) \geq g(0) + g'(0)t, \forall t}_{*}.\]

We compute

\[g'(0) = \left.\frac{d}{dt}g(t)\right\rvert_{t=0} = \nabla f^{\top} \left(\left.x_0 + tv\right\rvert_{t=0}\right)v = \nabla f^{\top}(x_0)v.\]

Making the appropriate substitutions for $g$ and $g'(0)$ into $*$ we get

\[f(x_0+tv) \geq f(x_0) + \nabla f^{\top}(x_0)tv.\]

Finally, we let $x = x_0 + tv$ and conclude that

\[f(x) \geq f(x_0) + \nabla f^{\top}(x_0)(x-x_0)\]

as desired.<br><br>
$\Leftarrow$: Now assume $f(x) \geq f(x_0) + \nabla f^{\top}(x_0)(x-x_0)$.<br><br>

Assume $x$ lies on an arbitrary line, $x = tu + (1-t)v$, where $u, v \in \textrm{Dom}(f)$. Let $t_0$ be chosen so that $x_0 = t_0u + (1-t_0)v$. <br><br>

Making the appropriate substitutions, we have
\[\begin{aligned}

f\left(tu + (1-t)v\right) &\geq f\left(t_0u + (1-t_0)v\right) \\ &+ \nabla f^{\top}\left(t_0u + (1-t_0)v\right)\left(tu + (1-t)v - t_0u - (1-t_0)v\right)

\end{aligned}\]

$\begin{aligned}

&= f\left(t_0u + (1-t_0)v\right) + \nabla f^{\top}\left(t_0u + (1-t_0)v\right)\left((t - t_0)u - (t-t_0)v\right) \\

&= f\left(t_0u + (1-t_0)v\right) + \nabla f^{\top}\left(t_0u + (1-t_0)v\right)\left((t - t_0)(u - v)\right)(**).

\end{aligned}$<br><br>
Let $g(t) = f\left(tu + (1-t)v\right)$. Then, we compute
\[g'(t) = \nabla f\left(tu + (1-t)v\right)(u-v).\]
Substituting $g$ and $g'$ into $**$, we obtain
\[g(t) \geq g(t_0) + g'(t_0)(t-t_0),\]
which means that $g$ is convex on the line defined by $u$ and $v$. Since $u$ and $v$ are arbitrary, it follows that $g$ is convex on any line through $\textrm{Dom}(f)$. Therefore, we conclude that $f$ is also convex. $\blacksquare$
</p>
</details>

> **Thm. First-Order Characterization of Convexity**<br>
> If $f$ is twice-differentiable, then it is convex iff $\nabla^2f(x) \succeq 0$, and strictly convex if $\nabla^2f(x) \succ 0$ for all $x \in \text{dom}(f)$.

<details>
 <summary><strong>See proof</strong>.</summary>
<p>
 Let $g(t) = f(x_0 + tv)$. Then $f$ is convex if and only if $g$ is convex for all $v, x_0 \in \textrm{Dom}(f)$, where we restrict the domain of $t$ so that $x_0 + tv \in \textrm{Dom}(f)$.
<br><br>
Since $g$ is a scalar function of $t$, it is convex if and only if
\[g''(t) \geq 0.\]
We compute
\[g''(t) = v^{\top}\nabla^2f(x_0 +tv)v.\]
Thus, $g$ is convex if and only if
\[\underbrace{v^{\top}\nabla^2f(x_0 + tv)v \geq 0}_{*}\]
for any $x_0 + tv \in \textrm{Dom}(f)$. Since $x_0$ can be chosen arbitrarily and $t$ can be made arbitrarily small, $*$ holds for every line $x_0 + tv \in \textrm{Dom}(f)$ if and only if
\[v^{\top}\nabla^2f(x)v \geq 0\]
for every $x, v \in \textrm{Dom}(f)$. In other words, $g$ is convex (and consequently $f$ is convex) if and only if $\nabla^2 f(x) \succeq 0$ for every $x \in \textrm{Dom}(f)$. $\blacksquare$
</p>
</details>

## Convex Optimization Problems
A **convex optimization** problem in stanadrd form is one in which:

 - $f_i$ is convex for $i = 0, \dots, m$
 - $h_j$ is affine, i.e., $h_j(x) = a^{\top} _jx + b_j$ for $j = 1, \dots, p$.

Often, we will simply replace the (scalar) equality constraints with the sole vector constraint $Ax=b$, where
\\[A = \begin{bmatrix} \vert & & \vert \\\\ a_1 & \dots & a_p \\\\ \vert & & \vert \end{bmatrix} \text{ and } b = \begin{bmatrix} b_1 \\\\ \vdots \\\\ b_p \end{bmatrix}.\\]

There are a few important properties of convex optimization problems:

- the feasible set of a convex optimization problem is a convex set.
  <details>
  <summary><strong>See proof</strong>.</summary>
  <p>
  Since $\mathcal{D}$ is the intersection of convex sets and is hence also convex. Thus, if $x, y \in \mathcal{D}$, then $z = \theta y + (1-\theta)x \in \mathcal{D}$, where $\theta \in [0,1]$.
  <br><br> 
  If $x, y \in \mathcal{X}$, then by definition, $f_i(x), f_i(y) \leq 0, i = 1, \dots, m$ and $h_j(x), h_j(y) = 0, j = 1, \dots, p$.
  <br><br> 
  For $i = 1, \dots, m$, we have
  \[f_i(z) = f_i\left(\theta y + (1-\theta)x\right) \leq \theta f_i(y) + (1-\theta)f_i(x) \leq 0.\]
  For $j = 1, \dots, p$, we have
  \[h_j(z) = a_j^{\top}z + b_j = a_j^{\top}\left(\theta y + (1-\theta)x\right)\]
  $\begin{aligned}
  &= \theta a_j^{\top}y + (1-\theta) a_j^{\top}x + b_j \\
  &= \theta \left(a_j^{\top}y - a_j^{\top}x\right) + a_j^{\top}x + b_j \\
  &= \theta \left(-b_j + b_j \right) - b_j + b_j \\ 
  &= 0.
  \end{aligned}$
  <br><br>
  Thus, $z \in \mathcal{X}$.
  </p>
  </details>
- any locally optimal point is also a globally optimal point.
  <details>
  <summary><strong>See proof</strong>.</summary>
  <p>
  Suppose $x$ is feasoble and locally optimal. Then, by definition, there exists an $R > 0$ such that if $f_0(z) < f_0(x)$, then $\|x - z\|_2 > R$ for every other feasible point $z$.
  <br><br>
  Now consider a feasible point $y$ such that $f_0(y) < f_0(x)$. Then, it must be that $\|x - y\|_2 > R$.
  <br><br>
  If we take $z = \theta y + (1-\theta)x$ where $\theta = R/\left(2\|x-y\|_2\right)$. Since $\|x-y\|_2 > R$, it follows that $0 \leq \theta \leq 1/2$, and so $z$ is a convex combination of two feasible points, and is hence, also feasible.
  <br><br>
  Since $f_0$ is convex, it follows that $f_0(z) = f_0\left(\theta y + (1-\theta)x\right) \leq \theta f_0(y) + (1-\theta)f_0(x) < f_0(x)$, where the last inequality follows from the fact that $f_0(y) < f_0(x)$.
  <br><br>
  However, we can show that $\|x-z\|_2 = R/2$. Indeed, we have
  \[\|x-z\|_2 = \left\| x - \left(\frac{R}{2\|x-y\|_2}y + \left(1 -\frac{R}{2\|x-y\|_2}\right)x\right) \right\|_2 \]
  $\begin{aligned}
  &= \left\|\frac{R}{2\|x-y\|_2}(x-y)\right\|_2 \\
  &= R/2.
  \end{aligned}$
  <br><br>
  To summarize, $z$ is a feasible point with $\|x-z\|_2 < R$ and $f_0(z) < f_0(x)$. Hence, $x$ cannot be locally optimal.
  </p>
  </details>

### First-Order Conditions
If $f_0$ is differentiable, then $x$ is locally optimal if and only if
\\[\nabla f_0(x)^{\top}(y-x) \geq 0, \forall y \in \mathcal{X}.\\]
<details>
<summary><strong>See proof</strong>.</summary>
<p>
We want to show that
\[f_0(x) \leq f_0(y), \forall y \in \mathcal{X} \Leftrightarrow f_0^{\top}(x)(x-y)\]
$\Rightarrow$: Assume $\nabla f_0^{\top}(x)(x-y) \geq 0$. Since $f_0$ is convex and differentiable, we have
\[f_0(y) \geq f_0(x) + \nabla f_0^{\top}(x)(x-y) \geq f_0(x).\]
$\Leftarrow$: Suppose $\exists y \in \mathcal{X}$ such that $\nabla f_0^{\top}(x)(y-x) < 0$, and define $z(\theta) = \theta y + (1-\theta) x$, where $\theta \in [0, 1]$. Since $\mathcal{X}$ is convex, it follows that $z(\theta) \in \mathcal{X}$. We compute
\[\nabla f_0^{\top}(x)(y-x) := \lim_{\theta\rightarrow 0}\frac{f_0\left(z(\theta)\right) - f_0(x)}{\theta} = \lim_{\theta\rightarrow 0}\frac{f_0\left(x + \theta (y-x)\right) - f_0(x)}{\theta}< 0.\]
Thus, for every $\epsilon > 0$, there exists a $\delta > 0$, such that
\[\frac{f_0\left(z(\theta)\right) - f_0(x)}{\theta}\]
However, this means that for every $R > 0$, 
</p>
</details>
   
### Second-Order Conditions
If $f_0$ is differentiable, then $x$ is locally optimal if and only if
\\[\nabla f(x)^{\top}(y-x) = 0 \Rightarrow (y-x)^{\top}f(x)(y-x) \geq 0.\\]
## Linear Programs
   A linear program is a convex optimization problem the form
   \\[\begin{aligned}\text{minimize } &c^{\top}x+d \\\\ \text{subject to } &Gx \leq 0 \\\\ &Ax + b = 0\end{aligned}\\]
   
## Non-Convex Optimization Problems
   
Finding $x^\*$ in non-convex optimization problems is considerably more difficult. However, we can derive a lower-bound for $f(x^\*)$. To do this, we first define two functions,
\\[L_f(x,\lambda,\mu) = f(x) + \sum_{i=1}^{m}\lambda_ig_i(x) + \sum_{j=1}^{n}\nu_jh_j(x),\\]
and
\\[\zeta_f(\lambda,\nu) = \underset{x \in \mathcal{X}}{\text{inf}}L_f(x,\lambda,\nu)\\]
where $\lambda_i \geq 0, i = 1, \dots, m$. These definitions may seem arbitrary, but the idea is that for any feasible $x$, $L_f(x,\lambda,\mu) \leq f(x)$, and so $\zeta_f(\lambda,\nu) \leq f(x^\*)$.
<br><br>   
Obviously, we want to choose $\lambda$ and $\nu$ so that $\zeta_f(\lambda,\nu)$ is as large as possible. This can be done by solving the maximization problem:
\\[\begin{aligned}\text{maximize } &\zeta_f(\lambda,\mu) \\\\ \text{subject to } &\lambda_i \geq 0, i = 1,\dots, m \end{aligned}\tag{DualOptPrb}\\]
We refer to (OptDualPrb) as the **dual problem** of (OptPrb). Since $\zeta_f$ as the point-wise infimum of infinitely many affine functions, it is concave.
