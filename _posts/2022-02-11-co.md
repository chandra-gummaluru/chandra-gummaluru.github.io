---
layout: post
title: "Course Notes: Convex Optimization"
author: "Chandra Gummaluru"
---

The following notes are heavily based on material developed by Professor Stephen Boyd for [EE364 taught at Stanford University](http://web.stanford.edu/class/ee364a/lectures.html). All typos are my own.

In the following, we will consider functions of the form, $f: \mathbb{R}^n \rightarrow \mathbb{R}$, where $\text{dom}(f) \subseteq \mathbb{R}^n$.
> **Def. Directional Derivative**:
> The rate of change of a function, $f: \mathbb{R}^n \rightarrow \mathbb{R}$, at some point $x \in \text{dom}(f)$ in the direction, $v \in \mathbb{R}^n$ is defined as
> \\[D_vf(x) := \lim_{t \rightarrow 0}\frac{f(x + tv) - f(x)}{t}.\\]
> We call $D_vf(x)$ the **derivative** of $f$ at $x$ in the direction, $v$. If $v = e_i$, then we call $D_{v}f$ the **partial derivative** of $f$ at $x$ w.r.t. $x_i$, and denote it as $\partial / \partial x_if(x)$.
> 
Computing a derivative from the aformentioned definition is often difficult. However, there are several theorems that can help.

> **Thm. Chain Rule**:
> Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$, $g: \mathbb{R} \rightarrow \mathbb{R}^n$, and define $h := f \circ g$. Then,
> \\[\frac{dh}{dt} = \sum_{i=1}^{n}\frac{\partial h}{\partial g_i}\frac{dg_i}{dt}.\\]

# Affine, Conic,and Convex Sets

## Affine Sets
An **affine combination** of the points, $x_1, \dots, x_n$ is of the form
\\[t_1x_1 + \dots t_nx_n\\] where $t_1 + \dots + t_n = 1$.

A set, $A$, is **affine** if it contains all affine combinations of its points.

Given a possibly non-affine set, $A$, the set of all possible affine combinations of its points is called its **affine hull**, and is denoted $\text{aff}(S)$. Note that $\text{aff}(S)$ is an affine set. 


## Conic Sets
A **conic combination** of the points, $x_1, \dots, x_n$ is of the form
\\[t_1x_1 + \dots t_nx_n\\] where $t_1,\dots,t_n \geq 0$.

A set, $C$, is **conic** if it contains all conic combinations of its points.

Given a possibly non-conic set, $S$, the set of all possible conic combinations of its points is called its **conic hull**, and is denoted $\text{conic}(S)$. Note that $\text{conic}(S)$ is a conic set. 

## Convex Sets
A **convex combination** of the points, $x_1, \dots, x_n$ is of the form
\\[t_1x_1 + \dots t_nx_n\\] where $t_1,\dots,t_n \geq 0$, and $t_1 + \dots +t_n = 1$.

A set, $C$, is **convex** if it contains all convex combinations of its points. Notice that all affine sets and conic sets are convex sets.

Given a possibly non-convex set, $S$, the set of all possible convex combinations of its points is called its **convex hull**, and is denoted $\text{conv}(S)$. The following relationship holds:

 - If $S$ is affine or conic, then it is convex.

## Examples of Convex Sets
We now consider some examples of convex sets.
### Hyper-planes and Half-Spaces
A **hyperplane**, is a set of the form, $\left\lbrace a^{\top}x = b \right\rbrace$, where $a \neq 0$. Hyper-planes are affine and (consequently) convex.

A **halfspace**, is a set of the form, $\left\lbrace a^{\top}x \leq b \right\rbrace$, where $a \neq 0$. Half-spaces are convex, but not conic or affine.

### Norm Balls and Cones
A **norm**, is an operator, $\\|\cdot\\|$, such that the following properties are satisfied:

 - $\\|x\\| \geq 0$ and $\\|x\\| = 0 \Leftrightarrow x = 0$
 - $\\|tx\\| = \|t\|\\|x\\|$, for any $t \in \mathbb{R}$
 - $\\|x + y \\| \leq \\|x\\| + \\|y\\|$

A **norm ball** of radius, $r$, with center $x_c$, is a set of the form
\\[\left\lbrace x \text{ s.t. } \|x - x_c\| \leq r\right\rbrace\\] Norm balls are convex.

A **norm cone** is a set of the form
\\[\left\lbrace x \text{ s.t. } \|x\| \leq t \right\rbrace \\] Norm cones are convex.

### Polyhedra
A **polyhedron** is the solution set of finitely many linear equalities and/or inequalities:
\\[Ax \preceq b \text{ and } Cx = d \\] In other words, it is a set of the form
\\[\left\lbrace x \text{ s.t. } Ax \preceq b, Cx = d\right\rbrace\\]
Polyhedra are convex.

## Operations that Preserve Convexity
To show that a set, $S$, is convex, one can resort to the definition, i.e., show that
\\[x_1, x_2 \in S, t \in [0, 1] \Rightarrow tx_1 +(1-t)x_2 \in S\\]However, this is often difficult to do in practice.

Another option is to show that $S$ can be formed from convex sets using operations that preserve convexity. Such operations include, but are not limited to:

 - **Intersections:** If $S_1$ and $S_2$ are convex, then so is $S_1 \cap S_2$.
 
 - **Affine Correspondences:** If $S$ is a convex set, and $f(x) = Ax+ b$, then
   - the image of $S$ under $f$, i.e., $f(S) = \left\lbrace f(x) \text{ s.t. } x \in S \right\rbrace$ is also convex
   - the inverse image of $S$ under $f$, i.e., $f^{-1}(S) = \left\lbrace x \text{ s. t. } f(x) \in S \right\rbrace$ is also convex.

# Convex Functions
> **Def. Convex Function**<br>
> A function, $f: \mathbb{R}^n \rightarrow \mathbb{R}$, is **convex**, iff $\text{dom}(f)$ is a convex set and
> \\[f(tx + (1-t)y) \leq tf(x)+(1-t)f(y)\\]for all $x, y \in \text{dom}(f)$ and $t \in [0,1]$.
> We say that $f$ is strictly convex if it is convex and
> \\[f(tx+(1-t)y) < tf(x) +(1-t)y\\]
> for $t \in (0,1)$.

Geometrically, for any convex function, $f$< the line segment between $(x, f(x))$ and $(y, f (y))$, which is the chord from $x$ to $y$, lies above the graph of $f$. 

## Necessary and Sufficient Conditions for Convexity
To check whether a function, $f$, is covnex or not, one can use the aformentioned definition. However, depending on $f$ this may be very tedious. Thus, it is useful to derive some other conditions that are both necessary and sufficient for $f$ to be convex.

> **Thm. Covnexity under Linear Restrictions** <br>
> Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ and define $g(t) := f(x_0 + tv)$. Then, $f$ is convex (in $x$) if and only if $g$ is convex (in $t$) for every $x_0 \in \text{dom}(f)$ and $v \in \mathbb{R}^n$.

We will make use of the aformentioned condition to derive two additional conditions.
> **Thm. First-Order Characterization of Convexity**<br>
> If $f$ is differentiable, then it is convex iff
> $\nabla f(y) \geq f(x) + \nabla f(x)^{\top}(y-x).$

<details>
 <summary><strong>See proof</strong>.</summary>
<p>
Let $g(t) = f(x_0 + tv)$. Then $f$ is convex if and only if $g$ is convex for all $v \in \mathbb{R}$, $x_0 \in \textrm{Dom}(f)$, where we restrict the domain of $t$ so that $x_0 + tv \in \textrm{Dom}(f)$.<br><br>

$\Rightarrow$: Since $g$ is a scalar function of $t$, if it is convex, then

\[\underbrace{g(t) \geq g(0) + g'(0)t, \forall t}_{*}.\]

We compute

\[g'(0) = \left.\frac{d}{dt}g(t)\right\rvert_{t=0} = \nabla f^{\top} \left(\left.x_0 + tv\right\rvert_{t=0}\right)v = \nabla f^{\top}(x_0)v.\]

Making the appropriate substitutions for $g$ and $g'(0)$ into $*$ we get

\[f(x_0+tv) \geq f(x_0) + \nabla f^{\top}(x_0)tv.\]

Finally, we let $x = x_0 + tv$ and conclude that

\[f(x) \geq f(x_0) + \nabla f^{\top}(x_0)(x-x_0)\]

as desired.<br><br>
$\Leftarrow$: Now assume $f(x) \geq f(x_0) + \nabla f^{\top}(x_0)(x-x_0)$.<br><br>

Assume $x$ lies on an arbitrary line, $x = tu + (1-t)v$, where $u, v \in \textrm{Dom}(f)$. Let $t_0$ be chosen so that $x_0 = t_0u + (1-t_0)v$. <br><br>

Making the appropriate substitutions, we have
\[\begin{aligned}

f\left(tu + (1-t)v\right) &\geq f\left(t_0u + (1-t_0)v\right) \\ &+ \nabla f^{\top}\left(t_0u + (1-t_0)v\right)\left(tu + (1-t)v - t_0u - (1-t_0)v\right)

\end{aligned}\]

$\begin{aligned}

&= f\left(t_0u + (1-t_0)v\right) + \nabla f^{\top}\left(t_0u + (1-t_0)v\right)\left((t - t_0)u - (t-t_0)v\right) \\

&= f\left(t_0u + (1-t_0)v\right) + \nabla f^{\top}\left(t_0u + (1-t_0)v\right)\left((t - t_0)(u - v)\right)(**).

\end{aligned}$<br><br>
Let $g(t) = f\left(tu + (1-t)v\right)$. Then, we compute
\[g'(t) = \nabla f\left(tu + (1-t)v\right)(u-v).\]
Substituting $g$ and $g'$ into $**$, we obtain
\[g(t) \geq g(t_0) + g'(t_0)(t-t_0),\]
which means that $g$ is convex on the line defined by $u$ and $v$. Since $u$ and $v$ are arbitrary, it follows that $g$ is convex on any line through $\textrm{Dom}(f)$. Therefore, we conclude that $f$ is also convex. $\blacksquare$
</p>
</details>

> **Thm. First-Order Characterization of Convexity**<br>
> If $f$ is twice-differentiable, then it is convex iff $\nabla^2f(x) \succeq 0$, and strictly convex if $\nabla^2f(x) \succ 0$ for all $x \in \text{dom}(f)$.

<details>
 <summary><strong>See proof</strong>.</summary>
<p>
 Let $g(t) = f(x_0 + tv)$. Then $f$ is convex if and only if $g$ is convex for all $v, x_0 \in \textrm{Dom}(f)$, where we restrict the domain of $t$ so that $x_0 + tv \in \textrm{Dom}(f)$.
<br><br>
Since $g$ is a scalar function of $t$, it is convex if and only if
\[g''(t) \geq 0.\]
We compute
\[g''(t) = v^{\top}\nabla^2f(x_0 +tv)v.\]
Thus, $g$ is convex if and only if
\[\underbrace{v^{\top}\nabla^2f(x_0 + tv)v \geq 0}_{*}\]
for any $x_0 + tv \in \textrm{Dom}(f)$. Since $x_0$ can be chosen arbitrarily and $t$ can be made arbitrarily small, $*$ holds for every line $x_0 + tv \in \textrm{Dom}(f)$ if and only if
\[v^{\top}\nabla^2f(x)v \geq 0\]
for every $x, v \in \textrm{Dom}(f)$. In other words, $g$ is convex (and consequently $f$ is convex) if and only if $\nabla^2 f(x) \succeq 0$ for every $x \in \textrm{Dom}(f)$. $\blacksquare$
</p>
</details>

# Optimization Problems
For simplicity, we will focus on minimization problems. A minimization problem is typically written as:
\\[\begin{equation}\begin{aligned}\text{minimize } &f(x) \\\\ \text{subject to } &g_i(x) \leq 0, i = 1,\dots, m \\\\ &h_j(x) =0, j = 1, \dots, n\end{aligned}\end{equation}\tag{OptPrb}\\]
where
 - $x \in \mathbb{R}^n$ is called the **optimization variable**
 - $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is called the **objective function**
 - $g_i: \mathbb{R}^n \rightarrow \mathbb{R}, i =1, \dots, m$ are called the **inequality constraints**, and
 - $h_j: \mathbb{R}^n \rightarrow \mathbb{R}, j =1, \dots n$ are called the **equality constraints**.

The $g_i$ and $h_j$ are the **explicit constraints**. However, there is also the **implicit constraint** that $x$ must be within intersection of the domains of $f, g_1 \dots, g_m, h_1, \dots, h_n$, i.e.,
\\[\mathcal{D} := \text{dom}(f) \cap \bigcap_{i=1}^{m}\text{dom}(g_i) \cap \bigcap_{j=1}^{n}\text{dom}(h_i).\\]

A point $x \in \mathcal{D}$, is **feasible** if it also satisfies the explicit constraints, i.e., iff
\\[g_i(x) \leq 0, i = 1, \dots, m,\\]
and
\\[h_j(x) = 0, j = 1, \dots, n.\\]
We will use $\mathcal{X}$ to denote the set of feasible points. Note that $\mathcal{X} \subseteq \mathcal{D}$.

A feasible point, $x^* \in \mathcal{X}$, is **globally optimal** if
\\[f(x^*) = \underset{x \in \mathcal{X}}{\text{inf}}\big\lbrace f(x) \big\rbrace.\\]

A feasible point, $x$, is **locally optimal** if there exists an $R > 0$, such that $x$ is globally optimal for the optimization problem

\\[\begin{aligned}\text{minimize } &f(z) \\\\ \text{subject to } &g_i(z) \leq 0, i = 1,\dots, m \\\\ &h_j(z) =0, j = 1, \dots, n \\\\ & \\|x- z\\|_2 \leq R \end{aligned}\\]

In other words, there must exist a neighbourhood around $x$ for which all points other than $x$ are sub-optimal.

A locally optimal point need not be globally optimal in general.

## Convex Optimization Problems
A **convex optimization** problem in stanadrd form is one in which:

 - $f_i$ is convex for $i = 0, \dots, m$
 - $h_j$ is affine, i.e., $h_j(x) = a^{\top} _jx + b_j$ for $j = 1, \dots, p$.

Often, we will simply replace the (scalar) equality constraints with the sole vector constraint $Ax=b$, where
\\[A = \begin{bmatrix} \vert & & \vert \\\\ a_1 & \dots & a_p \\\\ \vert & & \vert \end{bmatrix} \text{ and } b = \begin{bmatrix} b_1 \\\\ \vdots \\\\ b_p \end{bmatrix}.\\]

There are a few important properties of convex optimization problems:

- the feasible set of a convex optimization problem is a convex set.
  <details>
  <summary><strong>See proof</strong>.</summary>
  <p>
  Since $\mathcal{D}$ is the intersection of convex sets and is hence also convex. Thus, if $x, y \in \mathcal{D}$, then $z = \theta y + (1-\theta)x \in \mathcal{D}$, where $\theta \in [0,1]$.
  <br><br> 
  If $x, y \in \mathcal{X}$, then by definition, $f_i(x), f_i(y) \leq 0, i = 1, \dots, m$ and $h_j(x), h_j(y) = 0, j = 1, \dots, p$.
  <br><br> 
  For $i = 1, \dots, m$, we have
  \[f_i(z) = f_i\left(\theta y + (1-\theta)x\right) \leq \theta f_i(y) + (1-\theta)f_i(x) \leq 0.\]
  For $j = 1, \dots, p$, we have
  \[h_j(z) = a_j^{\top}z + b_j = a_j^{\top}\left(\theta y + (1-\theta)x\right)\]
  $\begin{aligned}
  &= \theta a_j^{\top}y + (1-\theta) a_j^{\top}x + b_j \\
  &= \theta \left(a_j^{\top}y - a_j^{\top}x\right) + a_j^{\top}x + b_j \\
  &= \theta \left(-b_j + b_j \right) - b_j + b_j \\ 
  &= 0.
  \end{aligned}$
  <br><br>
  Thus, $z \in \mathcal{X}$.
  </p>
  </details>
- any locally optimal point is also a globally optimal point.
  <details>
  <summary><strong>See proof</strong>.</summary>
  <p>
  Suppose $x$ is feasoble and locally optimal. Then, by definition, there exists an $R > 0$ such that if $f_0(z) < f_0(x)$, then $\|x - z\|_2 > R$ for every other feasible point $z$.
  <br><br>
  Now consider a feasible point $y$ such that $f_0(y) < f_0(x)$. Then, it must be that $\|x - y\|_2 > R$.
  <br><br>
  If we take $z = \theta y + (1-\theta)x$ where $\theta = R/\left(2\|x-y\|_2\right)$. Since $\|x-y\|_2 > R$, it follows that $0 \leq \theta \leq 1/2$, and so $z$ is a convex combination of two feasible points, and is hence, also feasible.
  <br><br>
  Since $f_0$ is convex, it follows that $f_0(z) = f_0\left(\theta y + (1-\theta)x\right) \leq \theta f_0(y) + (1-\theta)f_0(x) < f_0(x)$, where the last inequality follows from the fact that $f_0(y) < f_0(x)$.
  <br><br>
  However, we can show that $\|x-z\|_2 = R/2$. Indeed, we have
  \[\|x-z\|_2 = \left\| x - \left(\frac{R}{2\|x-y\|_2}y + \left(1 -\frac{R}{2\|x-y\|_2}\right)x\right) \right\|_2 \]
  $\begin{aligned}
  &= \left\|\frac{R}{2\|x-y\|_2}(x-y)\right\|_2 \\
  &= R/2.
  \end{aligned}$
  <br><br>
  To summarize, $z$ is a feasible point with $\|x-z\|_2 < R$ and $f_0(z) < f_0(x)$. Hence, $x$ cannot be locally optimal.
  </p>
  </details>

### First-Order Conditions
If $f_0$ is differentiable, then $x$ is locally optimal if and only if
\\[\nabla f_0(x)^{\top}(y-x) \geq 0, \forall y \in \mathcal{X}.\\]
<details>
<summary><strong>See proof</strong>.</summary>
<p>
We want to show that
\[f_0(x) \leq f_0(y), \forall y \in \mathcal{X} \Leftrightarrow f_0^{\top}(x)(x-y)\]
$\Rightarrow$: Assume $\nabla f_0^{\top}(x)(x-y) \geq 0$. Since $f_0$ is convex and differentiable, we have
\[f_0(y) \geq f_0(x) + \nabla f_0^{\top}(x)(x-y) \geq f_0(x).\]
$\Leftarrow$: Suppose $\exists y \in \mathcal{X}$ such that $\nabla f_0^{\top}(x)(y-x) < 0$, and define $z(\theta) = \theta y + (1-\theta) x$, where $\theta \in [0, 1]$. Since $\mathcal{X}$ is convex, it follows that $z(\theta) \in \mathcal{X}$. We compute
\[\nabla f_0^{\top}(x)(y-x) := \lim_{\theta\rightarrow 0}\frac{f_0\left(z(\theta)\right) - f_0(x)}{\theta} = \lim_{\theta\rightarrow 0}\frac{f_0\left(x + \theta (y-x)\right) - f_0(x)}{\theta}< 0.\]
Thus, for every $\epsilon > 0$, there exists a $\delta > 0$, such that
\[\frac{f_0\left(z(\theta)\right) - f_0(x)}{\theta}\]
However, this means that for every $R > 0$, 
</p>
</details>
   
### Second-Order Conditions
If $f_0$ is differentiable, then $x$ is locally optimal if and only if
\\[\nabla f(x)^{\top}(y-x) = 0 \Rightarrow (y-x)^{\top}f(x)(y-x) \geq 0.\\]
## Linear Programs
   A linear program is a convex optimization problem the form
   \\[\begin{aligned}\text{minimize } &c^{\top}x+d \\\\ \text{subject to } &Gx \leq 0 \\\\ &Ax + b = 0\end{aligned}\\]
   
## Non-Convex Optimization Problems
   
Finding $x^\*$ in non-convex optimization problems is considerably more difficult. However, we can derive a lower-bound for $f(x^\*)$. To do this, we first define two functions,
\\[L_f(x,\lambda,\mu) = f(x) + \sum_{i=1}^{m}\lambda_ig_i(x) + \sum_{j=1}^{n}\nu_jh_j(x),\\]
and
\\[\zeta_f(\lambda,\nu) = \underset{x \in \mathcal{X}}{\text{inf}}L_f(x,\lambda,\nu)\\]
where $\lambda_i \geq 0, i = 1, \dots, m$. These definitions may seem arbitrary, but the idea is that for any feasible $x$, $L_f(x,\lambda,\mu) \leq f(x)$, and so $\zeta_f(\lambda,\nu) \leq f(x^\*)$.
<br><br>   
Obviously, we want to choose $\lambda$ and $\nu$ so that $\zeta_f(\lambda,\nu)$ is as large as possible. This can be done by solving the maximization problem:
\\[\begin{aligned}\text{maximize } &\zeta_f(\lambda,\mu) \\\\ \text{subject to } &\lambda_i \geq 0, i = 1,\dots, m \end{aligned}\tag{DualOptPrb}\\]
We refer to (OptDualPrb) as the **dual problem** of (OptPrb). Since $\zeta_f$ as the point-wise infimum of infinitely many affine functions, it is concave.
