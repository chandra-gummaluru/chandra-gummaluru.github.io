---
layout: post
title: "Course Notes: Linear Control Theory"
author: "Chandra Gummaluru"
---

The following notes are heavily based on material developed by Professor Margret Chapman for [ECE557 taught at the University of Toronto]()
  
# Signals
> **Def. Continuous-Time Signal:** A **continuous-time signal** is a function, $u: \mathbb{R} \rightarrow \mathbb{R}$ where $u(t)$ is the signal's value at time, $t$.
 
We will often view a continuous-time signal, $x$, as linear combinations of basis signals, say
\\[u(t) = \sum\_{k}\hat{u}\_k\phi_k(t).\\]
Some continuous-time signals exhibit special properties which can often be exploited to simplify their analysis. We now define some of these properties.
<br><br>
We begin with the notion of even/odd-ness.
> **Def. Even/Odd Continuous-Time Signal:** A continuous-time signal, $u$, is **even** iff $u(t) = u(-t), \forall t \in \mathbb{R}$ and **odd** if $u(t) = -u(-t)$ for
> all $t \in \mathbb{R}$.

It turns out that any continuous-time signal, $u$, can be decomposed into a linear combination of even/odd components.

> **Thm. Even/Odd Decomposition of Continuous-Time Signals:** Any continuous-time signal, $u$, can be decomposed into odd and even components, namely,
> \\[u(t) = u^{\text{e}}(t) + u^{\text{o}}(t),\\]
> where
> \\[u^{\text{e}}(t) = \frac{1}{2}\left[u(t) + u(-t)\right] \text{ and } u^{\text{o}}(t) = \frac{1}{2}\left[u(t) - u(-t)\right].\\]

We consider a few important transformations of continuous-time signals.
> **Def. Continuous-Time Time Shifting:** A signal, $u\_{\tau}: \mathbb{R} \rightarrow \mathbb{R}$ is a **time-shift** of a signal, $u: \mathbb{R} \rightarrow \mathbb{R}$, if
> \\[u_{\tau}(t) = u(t-\tau), \forall t.\\]
> For $\tau > 0$, the time-shift is into the past, and for $\tau < 0$, it is into the future.

In theory, it is possible to measure a continuous-time signal exactly at any time, $t \in \mathbb{R}$. However, in practice we must often resort to computing a (possibly non-uniform) mean measurement of the signal within some bounded region, $R_t$ around $t$. More precisely, we may only compute
\\[\int_{R_t}\phi(\tau)u(\tau)d\tau,\\]
where $\phi: \mathbb{R} \rightarrow \mathbb{C}$ denotes the weighting over $R_t$.

Thus, rather than representing continuous-time signals as functions, we will use a new construct called a distribution.
> **Def. Distribution:** A **distribution**, $T: \mathcal{D} \rightarrow \mathbb{C}$ is a linear functional where $\mathcal{D}$ is a set of test functions on which $T$ acts. The notation $\langle T, \phi \rangle$ is used to denote the distribution, $T$, acting on the test function $\phi \in \mathcal{D}$.

> **Def. Equivalent Distributions:** Two distributions, $T_1$ and $T_2$ are equivalent, denoted $T_1 = T_2$ iff
> \\[\langle T_1, \phi \rangle = \langle T_2, \phi \rangle,\\]
> for any appropriate test function, $\phi$.

Many conventional functions can be represented as distributions.

> **Def. Distribution of a Function**: Any (locally integrable) function, $x$, may be represented as a distribution via the mapping
> \\[\langle T_x, \phi \rangle \mapsto \int_{\text{dom}{x}}x(t)\phi(t)dt,\\]
> where $T_x$ denotes the distributional analogue or \textbf{regular distribution} of $x$.
> In a common abuse of notation, $x$ is also used to denote its regular distribution, $T_x$.

We now consider transformations of distributions, which follow from requiring that they be consistent with the conventional ones for regular distributions.

- **Sum of Two Distributions:** Let $T_1$ and $T_2$ be two distributions. Then, we define $T_1 + T_2$ as the distribution such that \\[\langle T_1 + T_2, \phi \rangle = \langle T_1, \phi \rangle + \langle T_2, \phi \rangle.\\]

- **Time-Shift of a Distribution**: For any distribution, $T$, and $d \in \mathbb{R}$, we define the backward time-shift, $T_d$, as a distribution such that \\[\langle T_d, \phi \rangle = \langle T, \phi_{-d} \rangle.\\]

- **Product with a Smooth Function**: For any distribution, $T$, and smooth function, $s$, we define the distributional product $T \cdot s$ so that \\[\langle T \cdot s, \phi \rangle = \langle T, s \cdot \phi \rangle.\\]

# Systems
> **Def. Continuous-Time System:** A **continuous-time system** is an operator, $\mathcal{T}$ that maps one continuous-time signal, $u: \mathbb{R} \rightarrow \mathbb{R}$ to another continuous-time signal, $y = \mathcal{T} \lbrace u \rbrace$.

We now explore the various properties a continuous-time system might exhibit.
<br><br>
The most important of these is the so-called ``linearity'' property. Again, this is because we often represent signals as linear combinations of other signals.
> **Def. Linear Continuous-Time System:** A continuous-time system, $\mathcal{T}$, is **linear** iff for any input signals, $u\_1$ and $u\_2$, and scalars $a_1$ and $a_2$, we have
> \\[\mathcal{T}\lbrace a_1u_1(t) + a_2u_2(t) \rbrace = a_1\mathcal{T}\lbrace u_1(t) \rbrace + a_2\mathcal{T}\lbrace u_2(t) \rbrace.\\]

Often, the linear combination will consist of time-shifted versions of the same signal. As such, the so-called ``time-invariance'' property is equally important.

> **Def. Time-Invariant Continuous-Time Signal:** A continuous-time signal is **time-invariant** iff for any input $u$, if
\\[y(t) = \mathcal{T}\lbrace u(\tau) \rbrace(t),\\]
then
\\[y(t - t_0) = \mathcal{T}\lbrace u(\tau-t_0) \rbrace(t), \forall t_0.\\]

We conclude with some other (less important) properties.
> **Def. Causal Continuous-Time System:** A continuous-time system, $\mathcal{T}$ is **causal** if its response to any signal, $u: \mathbb{R} \rightarrow \mathbb{R}^m$, at some time, $t$, depends only on the value of $u$ at or before $t$
