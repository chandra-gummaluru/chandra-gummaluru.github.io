---
layout: post
title: "Course Notes: Linear Control Theory"
author: "Chandra Gummaluru"
---

The following notes are heavily based on material developed by Professor Margret Chapman for [ECE557 taught at the University of Toronto]()
  
# Signals
> **Def. Continuous-Time Signal:** A **continuous-time signal** is a function, $u: \mathbb{R} \rightarrow \mathbb{R}$ where $u(t)$ is the signal's value at time, $t$.
 
We will often view a continuous-time signal, $x$, as linear combinations of basis signals, say
\\[u(t) = \sum\_{k}\hat{u}\_k\phi_k(t).\\]
Some continuous-time signals exhibit special properties which can often be exploited to simplify their analysis. We now define some of these properties.
<br><br>
We begin with the notion of even/odd-ness.
> **Def. Even/Odd Continuous-Time Signal:** A continuous-time signal, $u$, is **even** iff $u(t) = u(-t), \forall t \in \mathbb{R}$ and **odd** if $u(t) = -u(-t)$ for
> all $t \in \mathbb{R}$.

It turns out that any continuous-time signal, $u$, can be decomposed into a linear combination of even/odd components.

> **Thm. Even/Odd Decomposition of Continuous-Time Signals:** Any continuous-time signal, $u$, can be decomposed into odd and even components, namely,
> \\[u(t) = u^{\text{e}}(t) + u^{\text{o}}(t),\\]
> where
> \\[u^{\text{e}}(t) = \frac{1}{2}\left[u(t) + u(-t)\right] \text{ and } u^{\text{o}}(t) = \frac{1}{2}\left[u(t) - u(-t)\right].\\]

We consider a few important transformations of continuous-time signals.
> **Def. Continuous-Time Time Shifting:** A signal, $u\_{\tau}: \mathbb{R} \rightarrow \mathbb{R}$ is a **time-shift** of a signal, $u: \mathbb{R} \rightarrow \mathbb{R}$, if
> \\[u_{\tau}(t) = u(t-\tau), \forall t.\\]
> For $\tau > 0$, the time-shift is into the past, and for $\tau < 0$, it is into the future.

In theory, it is possible to measure a continuous-time signal exactly at any time, $t \in \mathbb{R}$. However, in practice we must often resort to computing a (possibly non-uniform) mean measurement of the signal within some bounded region, $R_t$ around $t$. More precisely, we may only compute
\\[\int_{R_t}\phi(\tau)u(\tau)d\tau,\\]
where $\phi: \mathbb{R} \rightarrow \mathbb{C}^{p,p}$ denotes the weighting over $R_t$.

Thus, rather than representing continuous-time signals as functions, we will use a new construct called a distribution.
> **Def. Distribution:** A **distribution**, $T: \mathcal{D} \rightarrow \mathbb{C}$ is a linear functional where $\mathcal{D}$ is a set of test functions on which $T$ acts. The notation $\langle T, \phi \rangle$ is used to denote the distribution, $T$, acting on the test function $\phi \in \mathcal{D}$.

> **Def. Equivalent Distributions:** Two distributions, $T_1$ and $T_2$ are equivalent, denoted $T_1 = T_2$ iff
> \\[\langle T_1, \phi \rangle = \langle T_2, \phi \rangle,\\]
> for any appropriate test function, $\phi$.

Many conventional functions can be represented as distributions.

> **Def. Distribution of a Function**: Any (locally integrable) function, $x$, may be represented as a distribution via the mapping
> \\[\langle T_x, \phi \rangle \mapsto \int_{\text{dom}{x}}x(t)\phi(t)dt,\\]
> where $T_x$ denotes the distributional analogue or **regular distribution** of $x$.
> In a common abuse of notation, $x$ is also used to denote its regular distribution, $T_x$.

We now consider transformations of distributions, which follow from requiring that they be consistent with the conventional ones for regular distributions.

- **Sum of Two Distributions:** Let $T_1$ and $T_2$ be two distributions. Then, we define $T_1 + T_2$ as the distribution such that \\[\langle T_1 + T_2, \phi \rangle = \langle T_1, \phi \rangle + \langle T_2, \phi \rangle.\\]

- **Time-Shift of a Distribution**: For any distribution, $T$, and $d \in \mathbb{R}$, we define the backward time-shift, $T_d$, as a distribution such that \\[\langle T_d, \phi \rangle = \langle T, \phi_{-d} \rangle.\\]

- **Product with a Smooth Function**: For any distribution, $T$, and smooth function, $s$, we define the distributional product $T \cdot s$ so that \\[\langle T \cdot s, \phi \rangle = \langle T, s \cdot \phi \rangle.\\]

# Systems
A system is anything that takes in an input and produces an output.

> **Def. Continuous-Time System:** A **continuous-time system** is an operator, $\mathcal{T}$, that maps one continuous-time signal, $u: \mathbb{R}\_{+} \rightarrow \mathbb{R}^m$ to another continuous-time signal, $y = \mathcal{T}[u]$, where $y: \mathbb{R}\_{+} \in \mathbb{R}^p$.

We assume that the output of the system at time $t$, i.e., $y(t)$, given some input signal $u$, depends only on the value(s) of $u$ at or before $t$, i.e., $u(0),\dots,u(t)$. Such a system is said to be **causal**.

For a causal system, we can express $y(t)$ using a recursive definition:
\\[\begin{aligned}
\dot{x}(t) &= f(x(t), u(t),t) \\\\\\
y(t) &= g(x(t), u(t),t),
\end{aligned}\tag{DynSys}\\]
where $x:\mathbb{R}\_{+} \rightarrow \mathbb{R}^n, f: \mathbb{R}^{n} \times \mathbb{R}^{m}$ and $g: \mathbb{R}^{n} \times \mathbb{R}^{m} \rightarrow \mathbb{R}^{p}$. We refer to $x(t)$ as the state of the system at time, $t$.
<br><br>
We will consider systems in which $f$ and $g$ are constant w.r.t. $t$, or equivalently, systems in which
\\[y(t) = \mathcal{T}\left\[u(\tau) \right\](t) \Rightarrow y(t - t_0) = \mathcal{T}\left\[ u(\tau-t_0) \right\](t), \forall t_0.\\]
Such systems are called **time-invariant**.
<br><br>
If we knew $x(t), \forall t$, it would be relatively easy to compute $y(t), \forall t$. However, we often only know the initial state, $x_0 = x(0)$. To determine $x(t)$, we need to solve the initial-value problem
\\[
\begin{aligned}
\dot{x} &= f(x,u) \\\\\\
x(0) &= x_0.
\end{aligned}
\\]
This is difficult to do in general. However, if $x_0$ is such that $f(x_0, u) = 0$, the problem becomes trivial. Since $\dot{x} = 0$, the system's state does not change and we have $x(t) = x_0$. Such states are called equilibria.

> **Def. Equilibrium of a System:** Consider the system defined by (DynSys). A state, $\bar{x} \in \mathbb{R}^{n}$ is an \textbf{equilibrium} under the input $\bar{u}: \mathbb{R} \rightarrow \mathbb{R}^{m}$ iff
> \\[\dot{x} = f(\bar{x}, \bar{u}) = 0.\\]

Of course, identifying such equilibria is also not easy in general. However we will focus on systems in which $f$ and $g$ are _linear_ time-invariant functions, i.e.,
\\[\begin{aligned}
\dot{x}(t) &= Ax(t) + Bu(t) \\\\\\
y(t) &= Cx(t) + Du(t),
\end{aligned}\tag{LinDynSys}\\]
where $A \in \mathbb{R}^{n,n}, B \in \mathbb{R}^{n,m}, C \in \mathbb{R}^{p,n}, D \in \mathbb{R}^{p,m}$, or equivalently, systems in which for any input signals, $u\_1$ and $u\_2$, and scalars $a_1$ and $a_2$, we have
\\[\mathcal{T}\left[ a_1u_1(t) + a_2u_2(t) \right] = a_1\mathcal{T}\left[ u_1(t) \right] + a_2\mathcal{T}\left[ u_2(t) \right].\\]

We now explore the various properties a continuous-time system might exhibit.
<br><br>
The most important of these is the so-called ``linearity'' property. Again, this is because we often represent signals as linear combinations of other signals.

Often, the linear combination will consist of time-shifted versions of the same signal. As such, the so-called ``time-invariance'' property is equally important.


We will primarily focus on linear time-invariant (LTI) continuous-time systems.
<br><br>
LTI systems are particularly simple to study because their behaviour can be entirely characterized by their responses to a unit impulse, i.e., their impulse response.

> **Thm. Response of a Continuous-Time Linear System:** Consider a continuous-time LTI system with an impulse response, $h(t) = \mathcal{T} \lbrace \delta(t) \rbrace$. Then for any other continuous-time signal, $u: \mathbb{R} \rightarrow \mathbb{R}^m$,
> \\[\mathcal{T}\lbrace u(\tau) \rbrace(t) = (u * h)(t) := \int_{-\infty}^{\infty}h(t - \tau)u(\tau)d\tau,\\]
> where $h: \mathbb{R} \rightarrow \mathbb{R}^{p \times m}$.

The continuous-time convolution operator, $\*$, satisfies the following properties:
- **Distributivity:** $(h\_1 + h\_2)\*u = h\_1\*u + h\_2\*u$,
- **Commutativity:** $h \* u = h \* x$,
- **Associativity:** $(h\_1 \* h\_2) \* u = h\_1 \* (h\_2 \* u)$.

As a result, continuous-time LTI systems are also distributive, commutative, and associative.


> **Thm. Impulse Response of a Causal LTI System:** The impulse response, $h$, of a causal discrete-time LTI system is always such that \\[h(t) = 0, \forall t < 0.\\]

<details>
 <summary><strong>See proof</strong>.</summary>
<p>
Consider the response $y: \mathbb{R} \rightarrow \mathbb{R}^p$ of the system to an arbitrary continuous-time signal, $u: \mathbb{R} \rightarrow \mathbb{R}^m$. We have
\[y(t) = (x * h)(t) = (h * x)(t)\]
$\begin{aligned}
&= \int_{-\infty}^{\infty}h(\tau)u(t- \tau)d\tau \\
&= \int_{-\infty}^{0}h(\tau)u(t - \tau)d\tau + \int_{0}^{\infty}h(\tau)u(t-\tau)d\tau
\end{aligned}$
<br><br>
To satisfy causality, the first sum must evaluate to zero for any $u$, and so $h(t) = 0, \forall t < 0$. $\blacksquare$
</p>
</details>

> **Def. Impulse Response of a Memoryless LTI System:** The impulse response, $h$ of a memoryless continuous-time LTI system is always such that \\[h(t) = 0, \forall n \neq 0.\\]


<details>
 <summary><strong>See proof</strong>.</summary>
<p>
Consider the response $y: \mathbb{R} \rightarrow \mathbb{R}^p$ of the system to an arbitrary continuous-time signal, $u: \mathbb{R} \rightarrow \mathbb{R}^m$. We have
\[y(t) = (x * h)(t) = (h * x)(t)\]
$\begin{aligned}
&= \int_{-\infty}^{\infty}h(\tau)u(t- \tau)d\tau \\
&= \int_{-\infty}^{0}h(\tau)u(t - \tau)d\tau + \int_{0}^{\infty}h(\tau)u(t-\tau)d\tau
\end{aligned}$
<br><br>
To satisfy causality, the first sum must evaluate to zero for any $u$, and so $h(t) = 0, \forall t < 0$. $\blacksquare$
</p>
</details>

It turns out that exponential signals, i.e., those of the form, $e^{st}$, where $s \in \mathbb{C}^m$, are eigensignals of LTI systems.

> **Def. Response of a Continuous-Time LTI System to an Exponential:** The response of a continuous-time LTI system $\mathcal{T}$ to a signal of the form, $e^{st}$ is \\[\mathcal{T}\lbrace e^{st} \rbrace(t) = H(s)e^{st},\\]
where $H(s) = \mathscr{L}\lbrace h(t) \rbrace(s) \in \mathbb{C}^{p \times m}$ is called the **system function**.

<details>
 <summary><strong>See proof</strong>.</summary>
<p>
Let $u(t) = e^{st}$ where $s \in \mathbb{C}^{m}$. We have
\[y(t) = \mathcal{T}\lbrace u(t) \rbrace(t) = (h * u)(t)\]
$\begin{aligned}
&= \int_{-\infty}^{\infty}h(\tau)u(t-\tau)d\tau \\
&= \int_{-\infty}^{\infty}h(\tau)e^{s(t-\tau)}d\tau \\
&= \underbrace{\left(\int_{-\infty}^{\infty}h(\tau)e^{-s\tau}d\tau\right)}_{\mathscr{L}\lbrace h \rbrace(s)}e^{st}. \blacksquare
\end{aligned}$
</p>
</details>

> **Def. System Function:** Consider a continuous-time LTI system, $\mathcal{T}$, with a system function, $H$. Suppose $y$ is the output of $\mathcal{T}$ to a signal, $x$, i.e.,
> \\[y(t) = \mathcal{T}\lbrace u(t)\rbrace(t).\\]
> It follows that $Y(s) = H(s)U(s)$, where $U(s) = \mathscr{L}\lbrace u(t)\rbrace(s)$ and $Y(s) = \mathscr{L}\lbrace y(t)\rbrace(s)$.
