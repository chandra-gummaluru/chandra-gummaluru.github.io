---
layout: post
title: "Course Notes: Linear Algebra"
author: "Chandra Gummaluru"
---

## Linear Spaces and their Subspaces
To define the notion of a linear space, we must first define the notion of a field. Loosely speaking, a field is any set endowed with a sense of addition and multiplication. A formal definition is given below.

> **Def. Field**: A **field**, $\mathbb{F}$, is any set of elements endowed with the operations, $+: \mathbb{F} \times \mathbb{F} \rightarrow \mathbb{F}$ called addition, and $\cdot: \mathbb{F} \times \mathbb{F} \rightarrow \mathbb{F}$, called multiplication, such that the following properties are satisfied:
> - **commutativity**: $u+v = v + u$ and $u\cdot v = v \cdot u$ for all $u, v \in \mathbb{F}$.
> - **associativity**: $(u+v)+w = u+(v+w)$ and $(u\cdot v)\cdot w = u\cdot (v\cdot w)$ for all $u, v, w \in \mathbb{F}$.
> - **distributivity**: $u \cdot (v+w) = u \cdot v + u \cdot w$ for all $u, v, w \in \mathbb{F}$. 
> - **identities**: there exists $0 \in \mathbb{F}$, called the additive identity, and $1 \in \mathbb{F}$ called the multiplicative identity, such that $v + 0 = 1 \cdot v = v$ for all $v \in \mathbb{F}$.
> - **inverses**: for every $v \in \mathbb{F}$, there exists $-v, v^{-1} \in \mathbb{F}$, called the additive and multiplicative inverses, such that $v + (-v) = 0$ and $v \cdot v^{-1} = 1$. 
 
Linear spaces are defined over fields.

> **Def. Linear Space**: A **linear space**, $\mathcal{V}$ over a field, $\mathbb{F}$, is a set of elements endowed with the operations $+: \mathcal{V} \times \mathcal{V} \rightarrow \mathcal{V}$, called addition, and $\cdot: \mathcal{V} \times \mathbb{F} \rightarrow \mathcal{V}$, called scalar multiplication, such that the following properties are satisfied:
> - **commutativity of addition**: $u + v = v + u, \forall u, v \in \mathcal{V}$,
> - **associativity of addition**: $(u + v) + w = u + (v + w), \forall u, v, w \in \mathcal{V}$,
> - **distributivity**: $s(u+v) = su + sv$ and $u(s+t) = su + st, \forall u, v \in \mathcal{V}, s, t \in \mathcal{F}$,
> - **identities**: there exists $\textbf{0} \in \mathcal{V}$ called the additive identity, and $1 \in \mathbb{F}$ called the multiplicative identity, such that $v + \textbf{0} = 1v = v$  for all $v \in \mathcal{V}$,
> - **additive inverse**: $\forall v \in \mathcal{V}$ there exists $-v \in \mathcal{V}$, called the additive inverse, such that $v + (-v) = \textbf{0}$.

While multiplication in $\mathbb{F}$ must be commutative and associative, its extension in $\mathcal{V}$ need not be. Similarly, while $\mathbb{F}$ must contain multiplicative inverses for all of its elements, $\mathcal{V}$ need not. Consequently,  any field is a linear spaces over itself. However, not every linear space is a field.

> **Def. Linear Combination, Span, and Dimension**: Let $\mathcal{X}$ be a linear space over a field, $\mathbb{F}$. A **linear combination** of $x_1, \dots, x_m \in \mathcal{X}$ is of the form
> \\[\lambda_1x_1 + \dots + \lambda_mx_m, \lambda_i \in \mathbb{F}.\\]
> The **span** of $x_1, \dots, x_m$, denoted $\text{span}(x_1, \dots, x_m)$ is the set of all possible linear combinations of $x_1, \dots, x_m$, i.e.,
> \\[\text{span}(x_1, \dots, x_m) = \left\lbrace \sum_{i=1}^{m}\lambda_ix_i | \lambda_i \in \mathbb{F} \right\rbrace.\\]
> The smallest $m$ for which there exist a set of vectors, $x_1, \dots, x_m \in \mathcal{X}$ such that $\text{span}(x_1, \dots, x_m) = \mathcal{X}$ is called the **dimension** of $\mathcal{X}$, denoted $\text{dim}(\mathcal{X})$.

> **Def. Linear Independence and Basis Sets**: Let $\mathcal{X}$ be a linear space over a field, $\mathbb{F}$. We say $x^{(1)}, \dots, x^{(m)} \in \mathcal{X}$ are **linearly independent** if $\lambda_1x^{(1)} + \dots + \lambda_mx^{(m)} = 0$ only when $\lambda_1 = \dots = \lambda_m = 0$. A **basis** for $\mathcal{X}$ is any set of $\text{dim}(\mathcal{X})$ linearly independent elements within $\mathcal{X}$ that also span $\mathcal{X}$.
> <br><br>
> We refer to $\lambda_1, \dots, \lambda_m$ as the **coefficients** of $x$ under the basis $\mathscr{X}$.

Generally speaking, linear spaces can be defined using any object. We will primarily focus on linear spaces of vectors, matrices, and functions. We will go into more detail about each of these objects in following sections. We will see that many of the linear spaces we work with also have additional structure which make them subspaces.
> **Def. Subspace**: A linear space, $\mathcal{V}$, over a field $\mathbb{F}$, is a \textbf{subspace} if the following properties are satisfied:
> - **additive closure**: $u + v \in \mathcal{V}, \forall u, v \in \mathcal{V}$, and
> - **multiplicative closure**: $sv \in \mathcal{V}, \forall s \in \mathbb{F}, v \in \mathcal{V}$.

The beauty of a linear space that is also a subspace is that any properties enjoyed by its elements are preserved under the operations of the space. It turns out that both $\mathbb{R}$ and $\mathbb{C}$ are subspaces.
> **Sum of Subspaces**: Let $\mathcal{U}$, and $\mathcal{V}$ be subspaces of $\mathcal{X}$. We define their sum as
> \\[\mathcal{U} + \mathcal{V} := \left\lbrace u + v, u \in \mathcal{U}, v \in \mathcal{V} \right\rbrace.\\]
> If $\mathscr{U}$ is a basis for $\mathcal{U}$ and $\mathscr{V}$ is a basis for $\mathcal{V}$, then $\mathscr{U} \cup \mathscr{V}$ is a basis for $\mathcal{U} + \mathcal{V}$.

> **Def. Independent and Complement Subspaces**: Let $\mathcal{U}$ and $\mathcal{V}$ be subspaces of $\mathcal{X}$. We say that $\mathcal{U}$ and $\mathcal{V}$ are **independent** if $\mathcal{U} \cap \mathcal{V} = \left\lbrace \textbf{0} \right\rbrace$ and **complements** if $\mathcal{U} + \mathcal{V} = \mathcal{X}$.
> <br><br>
> It turns out that every subspace of a linear space has an independent complement.

### Norms and Normed Linear Spaces
> **Def. Norm**: Let $\mathcal{X}$ be a linear space over the field, $\mathbb{F}$. A function, $\|\cdot\|: \mathcal{X} \rightarrow \mathbb{F}$ is a **norm** iff the following properties are satisfied:
> -  **sub-additivity**: $\\|x + y \\| \leq \\|x\\| + \\|y\\|$ for all $x, y \in \mathcal{X}$.
> -  **absolute homogeneity**: $\\|sx\\| = \vert s\vert \\|x\\|$ for all $x \in \mathcal{X}$ and $s \in \mathbb{F}$.
> -  **non-negativity**: $\\|x\\| \geq 0$ for all $x \in \mathcal{X}$ and $\\|x\\| = 0 \Rightarrow x = \mathbf{0}$.

A linear space endowed with a norm is called a normed inner product space. We give a formal definition below.
> **Normed Linear Space**: A **normed linear space** is a linear space, $\mathcal{X}$ endowed with a norm, $\\|\cdot\\|:\mathcal{X} \rightarrow \mathbb{F}$.

### Inner Products and Inner Product Spaces
> **Def. Inner-Product**: Let $\mathcal{X}$ be a linear space over the field $\mathbb{F}$. An **inner-product** is a function, $\langle \cdot, \cdot \rangle: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{F}$ such that:
> -  **linearity**: $\langle sx, y \rangle = s\langle x, y\rangle$ and $\langle x + y, z \rangle = \langle x, y \rangle + \langle y, z \rangle$, for all $x, y, z \in \mathcal{X}$ and $s \in \mathbb{F}$.
> -  **positivity**: $\langle x, x \rangle \geq 0$ if $x \neq \mathbf{0}$.
> -  **conjugate symmetry**: $\langle x, y \rangle = \overline{\langle y, x \rangle}$.
> These properties imply that $\langle x, x \rangle = 0$ if and only if $x = \mathbf{0}$.

> **Def. Orthogonality**: Let $\mathcal{X}$ be an inner-product space. Two elements, $x, y \in \mathcal{X}$ are **orthogonal** iff $\langle x, y \rangle = \mathbf{0}$.

> **Thm. Mutual Orthogonality implies Linear Independence**: Let $\mathcal{X}$ be an inner-product space over a field, $\mathbb{F}$. The elements $x^{(1)}, \dots, x^{(n)} \in \mathcal{X}$ are linearly independent if and only if they are orthogonal

<details>
 <summary><strong>See proof</strong>.</summary>
<p>
Assume $x^{(1)}, \dots, x^{(n)}$ are mutually orthogonal, i.e., $\langle x^{(i)}, x^{(j)} \rangle = \mathbf{0}$ for all $i \neq j$, but that they are not linearly independent, i.e.,
\[\sum_{i=1}^{n}\lambda_ix^{(i)} = \mathbf{0}\]
and $\lambda_k \neq 0$ for some $k$. Taking the inner-product of both sides of the above equation with $x^{(j)}$, we have
\[\left\langle x^{(k)}, \sum_{i=1}^{n}\lambda_ix^{(i)}\right\rangle = \lambda_k\langle x^{(k)}, x^{(k)} \rangle = \langle x^{(k)}, \mathbf{0} \rangle = 0.\]
This is a contradiction since $\langle x^{(k)}, x^{(k)} \rangle \neq 0$.
<br>
$\blacksquare$
</p>
</details>

A linear space endowed with an inner product is called an inner product space. We give a formal definition below.
> **Def. Inner-Product Space**: An **inner product space** is a linear space, $\mathcal{X}$, endowed with an inner-product $\langle \cdot, \cdot \rangle: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{F}$.

It turns out that any inner-product $\langle \cdot, \cdot \rangle$ induces a norm, $\\|\cdot\\|$ through the mapping $\\|x\\| = \langle x, x \rangle$, and thus every inner product space is also a normed space.
> **Thm. Orthogonal Complement**: Let $\mathcal{V}$ be a subspace of an inner-product space $\mathcal{X}$. The **orthogonal complemen**t of $\mathcal{V}$, is
> \\[\mathcal{V}^{\top} := \left\lbrace x \in \mathcal{X} : \langle x, v \rangle = 0, \forall v \in \mathcal{V} \right\rbrace.\\]

<details>
 <summary><strong>See proof</strong>.</summary>
<p>
Clearly $\mathcal{V}$ and $\mathcal{V}^{\top}$ are independent. Indeed, $v \in (\mathcal{V}^{\top} \cap \mathcal{V}) \Leftrightarrow \langle v, v \rangle = 0 \Leftrightarrow v = 0$. $\blacksquare$
</p>
</details>


## Linear Transformations
In this section, we review linear transformations.
> **Def. Linear Transformation**: Let $\mathcal{X}$ and $\mathcal{Y}$ be linear spaces over the field $\mathbb{F}$. A **linear transformation**, $\mathbf{L}: \mathcal{X} \rightarrow \mathcal{Y}$, is a function satisfying
> \\[\mathbf{L}(x + \lambda y) = \mathbf{L}(x) + \lambda L(y),\\]
> for all $x, y \in \mathcal{X}$ and $\lambda \in \mathbb{F}$.

> **Def. Image, Kernel, and Rank** Let $\mathbf{L}: \mathcal{X} \rightarrow \mathcal{Y}$ be a linear transformation. If $\mathcal{V}$ is a subspace of $\mathcal{X}$, then the **image** of $\mathcal{V}$ under $\mathbf{L}$ is
> \\[\mathbf{L}(\mathcal{V})= \left\lbrace y \in \mathcal{V} : (\exists x \in \mathcal{X})y=\mathbf{L}(x)\right\rbrace.\\]
> The image of $\mathcal{X}$ under $\mathbf{L}$, i.e., $\mathbf{L}(\mathcal{X})$, is called the \textbf{image} of $\mathbf{L}$, and is also denoted as $\text{img}{\mathbf{L}}$. The **kernel** of $\mathbf{L}$, denoted $\text{ker}{\mathbf{L}}$, is the set of elements, $x \in \mathcal{X}$, such that $\mathbf{L}(x) = \mathbf{0}$, i.e.,
> \\[\text{ker}{\mathbf{L}} = \left\lbrace x \in \mathcal{X} : \mathbf{L}(x) = \mathbf{0} \right\rbrace.\\]
> It turns out that $\mathbf{L}(\mathcal{V})$ is a subspace of $\mathcal{Y}$ and $\text{ker}{\mathbf{L}}$ is a subspace of $\mathcal{X}$. The **rank** of $\mathbf{L}$ is the dimension of its image, i.e., $\text{rank}{\mathbf{L}} = \text{dim}{(\text{img}{\mathbf{L}})}$.

> **Def. Injective, Surjective, and Bijective Transformations**: A linear transformation, $\mathbf{L}: \mathcal{X} \rightarrow \mathcal{Y}$ is **injective** (or one-to-one) if
> \\[\mathbf{L}(x^{(1)}) = \mathbf{L}(x^{(2)}) \Leftrightarrow x^{(1)} = x^{(2)}, \forall x^{(1)}, x^{(2)} \in \mathcal{X},\\]
> **surjective** (or onto) if $\text{img}{\mathbf{L}} = \mathcal{Y}$, and \textbf{bijective} if it is both injective and surjective. If there exists some bijective
> transformation between $\mathcal{X}$ and $\mathcal{Y}$, then we say that $\mathcal{X}$ and $\mathcal{Y}$ are isomorphisms.

> **Thm. Injectiveness, Surjectiveness, Invertibility, and Rank**:
> For any linear transformation, $\mathbf{L}: \mathcal{X} \rightarrow \mathcal{Y}$, we have
\\[\text{dim}\left(\text{img}{\mathbf{L}}\right) + \text{dim}\left(\text{ker}{\mathbf{L}}\right) = \text{dim}{\mathcal{X}}.\\] Moreover, $\mathbf{L}$ is
> -  injective if and only if $\text{ker}{\mathbf{L}} = \left\lbrace \mathbf{0} \right\rbrace$, or equivalently, if $\text{dim}\left(\text{ker}{\mathbf{L}}\right) = 0$,
> -  and surjective if and only if $\text{rank}{\mathbf{L}} := \text{dim}\left(\text{img}{\mathbf{L}}\right) = \text{dim}{\mathcal{Y}}$.
> <br>
> It follows that if $\mathbf{L}$ is bijective, then $\text{dim}{\mathcal{X}} = \text{dim}{\mathcal{Y}}$. However, the converse is not necessarily true.

Suppose all elements in $\mathcal{X}$ are expressed using the basis $\mathscr{X} = \left\lbrace x^{(1)}, \dots, x^{(m)} \right\rbrace$ and $x = \sum_{i = 1}^{m}x_ix^{(i)}, x \in \mathcal{X}$. Our goal is to find an expression for $\mathbf{L}$. To that end, consider applying $\mathbf{L}$ on $x$, i.e., $\mathbf{L}(x)$. We have
\\[\mathbf{L}(x) = \mathbf{L}\left(\sum_{i = 1}^{m}x_ix^{(i)}\right) = \sum_{i = 1}^{m}\mathbf{L}\left(x_ix^{(i)}\right) = \sum_{i = 1}^{m}x_i\mathbf{L}\left(x^{(i)}\right).\\]

In other words, the linear transformation of any element in $\mathcal{X}$ can be expressed as a linear combination of the linear transformations of the basis elements of $\mathcal{X}$. Now suppose that all elements in $\mathcal{Y}$ are expressed using the basis $\mathscr{Y} = \left\lbrace y^{(1)}, \dots, y^{(n)} \right\rbrace$ and that $\mathbf{L}\left(x^{(i)}\right) = \sum_{j=1}^{n}t_{j,i}y^{(i)}$. It follows that
\\[\mathbf{L}(x) = \sum_{i=1}^{m}x_i\left(\sum_{j=1}^{n}t_{j,i}y^{(j)}\right) = \sum_{j=1}^{n}\left(\sum_{i=1}^{m}x_it_{j,i}\right)y^{(j)}.\\]
Observe that each coefficient of $\mathbf{L}(x)$ under $\mathscr{Y}$ is a linear combination of the coefficients of $x$ under $\mathscr{X}$. We see that to define a linear transformation, $\mathbf{L}: \mathcal{X} \rightarrow \mathcal{Y}$, using the bases $\mathscr{X}$ and $\mathscr{Y}$, we need to find the element in $\mathcal{Y}$ (expressed in $\mathscr{Y}$) that each basis element in $\mathcal{X}$ (expressed using $\mathscr{X}$) is mapped to.
<br><br>
Suppose we wish to represent the same transformation but using different bases. In particular, suppose we wish to use $\mathscr{A}$ as the basis for $\mathcal{X}$ and $\mathscr{B}$ as the basis for $Y$. If $\mathbf{L}\_{\mathscr{X},\mathscr{Y}}$ represents the linear transformation in the original bases and $\mathbf{L}\_{\mathscr{A},\mathscr{B}}$ represents it in the new bases, we have
\\[\mathbf{L}_{\mathscr{A},\mathscr{B}} = \mathbf{T}_{\mathscr{Y},\mathscr{B}} \circ \mathbf{L}_{\mathscr{X},\mathscr{Y}} \circ \mathbf{T}_{\mathscr{A},\mathscr{X}},\\]
where $\mathbf{T}_{\mathscr{A},\mathscr{X}}$ is the change-of-basis transformation from $\mathscr{A}$ to $\mathscr{X}$ and $\mathbf{T}_{\mathscr{Y},\mathscr{B}}$ is the change-of-basis transformation from $\mathscr{Y}$ to $\mathscr{B}$. A change-of-basis transformation is always linear. 
