---
layout: post
title: "Course Notes: Linear Algebra"
author: "Chandra Gummaluru"
---

## Linear Spaces and their Subspaces
To define the notion of a linear space, we must first define the notion of a field. Loosely speaking, a field is any set endowed with a sense of addition and multiplication. A formal definition is given below.

> **Def. Field**: A **field**, $\mathbb{F}$, is any set of elements endowed with the operations, $+: \mathbb{F} \times \mathbb{F} \rightarrow \mathbb{F}$ called addition, and $\cdot: \mathbb{F} \times \mathbb{F} \rightarrow \mathbb{F}$, called multiplication, such that the following properties are satisfied:
> - **commutativity**: $u+v = v + u$ and $u\cdot v = v \cdot u$ for all $u, v \in \mathbb{F}$.
> - **associativity**: $(u+v)+w = u+(v+w)$ and $(u\cdot v)\cdot w = u\cdot (v\cdot w)$ for all $u, v, w \in \mathbb{F}$.
> - **distributivity**: $u \cdot (v+w) = u \cdot v + u \cdot w$ for all $u, v, w \in \mathbb{F}$. 
> - **identities**: there exists $0 \in \mathbb{F}$, called the additive identity, and $1 \in \mathbb{F}$ called the multiplicative identity, such that $v + 0 = 1 \cdot v = v$ for all $v \in \mathbb{F}$.
> - **inverses**: for every $v \in \mathbb{F}$, there exists $-v, v^{-1} \in \mathbb{F}$, called the additive and multiplicative inverses, such that $v + (-v) = 0$ and $v \cdot v^{-1} = 1$. 
 
Linear spaces are defined over fields.

> **Def. Linear Space**: A **linear space**, $\mathcal{V}$ over a field, $\mathbb{F}$, is a set of elements endowed with the operations $+: \mathcal{V} \times \mathcal{V} \rightarrow \mathcal{V}$, called addition, and $\cdot: \mathcal{V} \times \mathbb{F} \rightarrow \mathcal{V}$, called scalar multiplication, such that the following properties are satisfied:
> - **commutativity of addition**: $u + v = v + u, \forall u, v \in \mathcal{V}$,
> - **associativity of addition**: $(u + v) + w = u + (v + w), \forall u, v, w \in \mathcal{V}$,
> - **distributivity**: $s(u+v) = su + sv$ and $u(s+t) = su + st, \forall u, v \in \mathcal{V}, s, t \in \mathcal{F}$,
> - **identities**: there exists $\textbf{0} \in \mathcal{V}$ called the additive identity, and $1 \in \mathbb{F}$ called the multiplicative identity, such that $v + \textbf{0} = 1v = v$  for all $v \in \mathcal{V}$,
> - **additive inverse**: $\forall v \in \mathcal{V}$ there exists $-v \in \mathcal{V}$, called the additive inverse, such that $v + (-v) = \textbf{0}$.

While multiplication in $\mathbb{F}$ must be commutative and associative, its extension in $\mathcal{V}$ need not be. Similarly, while $\mathbb{F}$ must contain multiplicative inverses for all of its elements, $\mathcal{V}$ need not. Consequently,  any field is a linear spaces over itself. However, not every linear space is a field.

> **Def. Linear Combination, Span, and Dimension**: Let $\mathcal{X}$ be a linear space over a field, $\mathbb{F}$. A **linear combination** of $x_1, \dots, x_m \in \mathcal{X}$ is of the form
> \\[\lambda_1x_1 + \dots + \lambda_mx_m, \lambda_i \in \mathbb{F}.\\]
> The **span** of $x_1, \dots, x_m$, denoted $\text{span}(x_1, \dots, x_m)$ is the set of all possible linear combinations of $x_1, \dots, x_m$, i.e.,
> \\[\text{span}(x_1, \dots, x_m) = \left\lbrace \sum_{i=1}^{m}\lambda_ix_i | \lambda_i \in \mathbb{F} \right\rbrace.\\]
> The smallest $m$ for which there exist a set of vectors, $x_1, \dots, x_m \in \mathcal{X}$ such that $\text{span}(x_1, \dots, x_m) = \mathcal{X}$ is called the **dimension** of $\mathcal{X}$, denoted $\text{dim}(\mathcal{X})$.

> **Def. Linear Independence and Basis Sets**: Let $\mathcal{X}$ be a linear space over a field, $\mathbb{F}$. We say $x^{(1)}, \dots, x^{(m)} \in \mathcal{X}$ are **linearly independent** if $\lambda_1x^{(1)} + \dots + \lambda_mx^{(m)} = 0$ only when $\lambda_1 = \dots = \lambda_m = 0$. A **basis** for $\mathcal{X}$ is any set of $\text{dim}(\mathcal{X})$ linearly independent elements within $\mathcal{X}$ that also span $\mathcal{X}$.
> <br><br>
> We refer to $\lambda_1, \dots, \lambda_m$ as the **coefficients** of $x$ under the basis $\mathscr{X}$.

Generally speaking, linear spaces can be defined using any object. We will primarily focus on linear spaces of vectors, matrices, and functions. We will go into more detail about each of these objects in following sections. We will see that many of the linear spaces we work with also have additional structure which make them subspaces.
> **Def. Subspace**: A linear space, $\mathcal{V}$, over a field $\mathbb{F}$, is a \textbf{subspace} if the following properties are satisfied:
> - **additive closure**: $u + v \in \mathcal{V}, \forall u, v \in \mathcal{V}$, and
> - **multiplicative closure**: $sv \in \mathcal{V}, \forall s \in \mathbb{F}, v \in \mathcal{V}$.

The beauty of a linear space that is also a subspace is that any properties enjoyed by its elements are preserved under the operations of the space. It turns out that both $\mathbb{R}$ and $\mathbb{C}$ are subspaces.
> **Sum of Subspaces**: Let $\mathcal{U}$, and $\mathcal{V}$ be subspaces of $\mathcal{X}$. We define their sum as
> \\[\mathcal{U} + \mathcal{V} := \left\lbrace u + v, u \in \mathcal{U}, v \in \mathcal{V} \right\rbrace.\\]
> If $\mathscr{U}$ is a basis for $\mathcal{U}$ and $\mathscr{V}$ is a basis for $\mathcal{V}$, then $\mathscr{U} \cup \mathscr{V}$ is a basis for $\mathcal{U} + \mathcal{V}$.

> **Def. Independent and Complement Subspaces**: Let $\mathcal{U}$ and $\mathcal{V}$ be subspaces of $\mathcal{X}$. We say that $\mathcal{U}$ and $\mathcal{V}$ are **independent** if $\mathcal{U} \cap \mathcal{V} = \left\lbrace \textbf{0} \right\rbrace$ and **complements** if $\mathcal{U} + \mathcal{V} = \mathcal{X}$.
> <br><br>
> It turns out that every subspace of a linear space has an independent complement.

### Norms and Normed Linear Spaces
> **Def. Norm**: Let $\mathcal{X}$ be a linear space over the field, $\mathbb{F}$. A function, $\|\cdot\|: \mathcal{X} \rightarrow \mathbb{F}$ is a **norm** iff the following properties are satisfied:
> -  **sub-additivity**: $\\|x + y \\| \leq \\|x\\| + \\|y\\|$ for all $x, y \in \mathcal{X}$.
> -  **absolute homogeneity**: $\\|sx\\| = \vert s\vert \\|x\\|$ for all $x \in \mathcal{X}$ and $s \in \mathbb{F}$.
> -  **non-negativity**: $\\|x\\| \geq 0$ for all $x \in \mathcal{X}$ and $\\|x\\| = 0 \Rightarrow x = \mathbf{0}$.

A linear space endowed with a norm is called a normed inner product space. We give a formal definition below.
> **Normed Linear Space**: A **normed linear space** is a linear space, $\mathcal{X}$ endowed with a norm, $\\|\cdot\\|:\mathcal{X} \rightarrow \mathbb{F}$.

### Inner Products and Inner Product Spaces
> **Def. Inner-Product**: Let $\mathcal{X}$ be a linear space over the field $\mathbb{F}$. An **inner-product** is a function, $\langle \cdot, \cdot \rangle: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{F}$ such that:
> -  **linearity**: $\langle sx, y \rangle = s\langle x, y\rangle$ and $\langle x + y, z \rangle = \langle x, y \rangle + \langle y, z \rangle$, for all $x, y, z \in \mathcal{X}$ and $s \in \mathbb{F}$.
> -  **positivity**: $\langle x, x \rangle \geq 0$ if $x \neq \mathbf{0}$.
> -  **conjugate symmetry**: $\langle x, y \rangle = \overline{\langle y, x \rangle}$.
> These properties imply that $\langle x, x \rangle = 0$ if and only if $x = \mathbf{0}$.

> **Def. Orthogonality**: Let $\mathcal{X}$ be an inner-product space. Two elements, $x, y \in \mathcal{X}$ are **orthogonal** iff $\langle x, y \rangle = \mathbf{0}$.

> **Thm. Mutual Orthogonality implies Linear Independence**: Let $\mathcal{X}$ be an inner-product space over a field, $\mathbb{F}$. The elements $x^{(1)}, \dots, x^{(n)} \in \mathcal{X}$ are linearly independent if and only if they are orthogonal

<details>
 <summary><strong>See proof</strong>.</summary>
<p>
Assume $x^{(1)}, \dots, x^{(n)}$ are mutually orthogonal, i.e., $\langle x^{(i)}, x^{(j)} \rangle = \mathbf{0}$ for all $i \neq j$, but that they are not linearly independent, i.e.,
\[\sum_{i=1}^{n}\lambda_ix^{(i)} = \mathbf{0}\]
and $\lambda_k \neq 0$ for some $k$. Taking the inner-product of both sides of the above equation with $x^{(j)}$, we have
\[\left\langle x^{(k)}, \sum_{i=1}^{n}\lambda_ix^{(i)}\right\rangle = \lambda_k\langle x^{(k)}, x^{(k)} \rangle = \langle x^{(k)}, \mathbf{0} \rangle = 0.\]
This is a contradiction since $\langle x^{(k)}, x^{(k)} \rangle \neq 0$.
<br>
$\blacksquare$
</p>
</details>

A linear space endowed with an inner product is called an inner product space. We give a formal definition below.
> **Def. Inner-Product Space**: An **inner product space** is a linear space, $\mathcal{X}$, endowed with an inner-product $\langle \cdot, \cdot \rangle: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{F}$.

It turns out that any inner-product $\langle \cdot, \cdot \rangle$ induces a norm, $\\|\cdot\\|$ through the mapping $\\|x\\| = \langle x, x \rangle$, and thus every inner product space is also a normed space.
> **Thm. Orthogonal Complement**: Let $\mathcal{V}$ be a subspace of an inner-product space $\mathcal{X}$. The **orthogonal complemen**t of $\mathcal{V}$, is
> \\[\mathcal{V}^{\top} := \left\lbrace x \in \mathcal{X} : \langle x, v \rangle = 0, \forall v \in \mathcal{V} \right\rbrace.\\]

<details>
 <summary><strong>See proof</strong>.</summary>
<p>
Clearly $\mathcal{V}$ and $\mathcal{V}^{\top}$ are independent. Indeed, $v \in (\mathcal{V}^{\top} \cap \mathcal{V}) \Leftrightarrow \langle v, v \rangle = 0 \Leftrightarrow v = 0$. $\blacksquare$
</p>
</details>


## Linear Transformations
In this section, we review linear transformations.
> **Def. Linear Transformation**: Let $\mathcal{X}$ and $\mathcal{Y}$ be linear spaces over the field $\mathbb{F}$. A **linear transformation**, $\mathbf{L}: \mathcal{X} \rightarrow \mathcal{Y}$, is a function satisfying
> \\[\mathbf{L}(x + \lambda y) = \mathbf{L}(x) + \lambda L(y),\\]
> for all $x, y \in \mathcal{X}$ and $\lambda \in \mathbb{F}$.

> **Def. Image, Kernel, and Rank** Let $\mathbf{L}: \mathcal{X} \rightarrow \mathcal{Y}$ be a linear transformation. If $\mathcal{V}$ is a subspace of $\mathcal{X}$, then the **image** of $\mathcal{V}$ under $\mathbf{L}$ is
> \\[\mathbf{L}(\mathcal{V})= \left\lbrace y \in \mathcal{V} : (\exists x \in \mathcal{X})y=\mathbf{L}(x)\right\rbrace.\\]
> The image of $\mathcal{X}$ under $\mathbf{L}$, i.e., $\mathbf{L}(\mathcal{X})$, is called the \textbf{image} of $\mathbf{L}$, and is also denoted as $\text{img}{L}$. The \textbf{kernel} of $\mathbf{L}$, denoted $\text{ker}{\mathbf{L}}$, is the set of elements, $x \in \mathcal{X}$, such that $\mathbf{L}(x) = \mathbf{0}$, i.e.,
> \\[\text{ker}{\mathbf{L}} = \left\lbrace x \in \mathcal{X} : \mathbf{L}(x) = \bm{0} \right\rbrace.\\]
> It turns out that $\mathbf{L}(\mathcal{V})$ is a subspace of $\mathcal{Y}$ and $\text{ker}{\mathbf{L}}$ is a subspace of $\mathcal{X}$. The \textbf{rank} of $\mathbf{L}$ is the dimension of its image, i.e., $\text{rank}{\mathbf{L}} = \text{dim}{(\text{img}{\mathbf{L}})}$.
