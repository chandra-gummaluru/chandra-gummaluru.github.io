---
layout: post
title: "Course Notes: Linear Algebra"
author: "Chandra Gummaluru"
---

## Linear Spaces and their Subspaces
To define the notion of a linear space, we must first define the notion of a field. Loosely speaking, a field is any set endowed with a sense of addition and multiplication. A formal definition is given below.

> **Def. Field**: A **field**, $\mathbb{F}$, is any set of elements endowed with the operations, $+: \mathbb{F} \times \mathbb{F} \rightarrow \mathbb{F}$ called addition, and $\cdot: \mathbb{F} \times \mathbb{F} \rightarrow \mathbb{F}$, called multiplication, such that the following properties are satisfied:
> - **commutativity**: $u+v = v + u$ and $u\cdot v = v \cdot u$ for all $u, v \in \mathbb{F}$.
> - **associativity**: $(u+v)+w = u+(v+w)$ and $(u\cdot v)\cdot w = u\cdot (v\cdot w)$ for all $u, v, w \in \mathbb{F}$.
> - **distributivity**: $u \cdot (v+w) = u \cdot v + u \cdot w$ for all $u, v, w \in \mathbb{F}$. 
> - **identities**: there exists $0 \in \mathbb{F}$, called the additive identity, and $1 \in \mathbb{F}$ called the multiplicative identity, such that $v + 0 = 1 \cdot v = v$ for all $v \in \mathbb{F}$.
> - **inverses**: for every $v \in \mathbb{F}$, there exists $-v, v^{-1} \in \mathbb{F}$, called the additive and multiplicative inverses, such that $v + (-v) = 0$ and $v \cdot v^{-1} = 1$. 
 
Linear spaces are defined over fields.

> **Def. Linear Space**: A **linear space**, $\mathcal{V}$ over a field, $\mathbb{F}$, is a set of elements endowed with the operations $+: \mathcal{V} \times \mathcal{V} \rightarrow \mathcal{V}$, called addition, and $\cdot: \mathcal{V} \times \mathbb{F} \rightarrow \mathcal{V}$, called scalar multiplication, such that the following properties are satisfied:
> - **commutativity of addition**: $u + v = v + u, \forall u, v \in \mathcal{V}$,
> - **associativity of addition**: $(u + v) + w = u + (v + w), \forall u, v, w \in \mathcal{V}$,
> - **distributivity**: $s(u+v) = su + sv$ and $u(s+t) = su + st, \forall u, v \in \mathcal{V}, s, t \in \mathcal{F}$,
> - **identities**: there exists $\textbf{0} \in \mathcal{V}$ called the additive identity, and $1 \in \mathbb{F}$ called the multiplicative identity, such that $v + \textbf{0} = 1v = v$  for all $v \in \mathcal{V}$,
> - **additive inverse**: $\forall v \in \mathcal{V}$ there exists $-v \in \mathcal{V}$, called the additive inverse, such that $v + (-v) = \textbf{0}$.

While multiplication in $\mathbb{F}$ must be commutative and associative, its extension in $\mathcal{V}$ need not be. Similarly, while $\mathbb{F}$ must contain multiplicative inverses for all of its elements, $\mathcal{V}$ need not. Consequently,  any field is a linear spaces over itself. However, not every linear space is a field.

> **Def. Linear Combination, Span, and Dimension**: Let $\mathcal{X}$ be a linear space over a field, $\mathbb{F}$. A **linear combination** of $x_1, \dots, x_m \in \mathcal{X}$ is of the form
> \\[\lambda_1x_1 + \dots + \lambda_mx_m, \lambda_i \in \mathbb{F}.\\]
> The **span** of $x_1, \dots, x_m$, denoted $\text{span}(x_1, \dots, x_m)$ is the set of all possible linear combinations of $x_1, \dots, x_m$, i.e.,
> \\[\text{span}(x_1, \dots, x_m) = \left\lbrace \sum_{i=1}^{m}\lambda_ix_i | \lambda_i \in \mathbb{F} \right\rbrace.\\]
> The smallest $m$ for which there exist a set of vectors, $x_1, \dots, x_m \in \mathcal{X}$ such that $\text{span}(x_1, \dots, x_m) = \mathcal{X}$ is called the **dimension** of $\mathcal{X}$, denoted $\text{dim}(\mathcal{X})$.

> **Def. Linear Independence and Basis Sets**: Let $\mathcal{X}$ be a linear space over a field, $\mathbb{F}$. We say $x^{(1)}, \dots, x^{(m)} \in \mathcal{X}$ are **linearly independent** if $\lambda_1x^{(1)} + \dots + \lambda_mx^{(m)} = 0$ only when $\lambda_1 = \dots = \lambda_m = 0$. A **basis** for $\mathcal{X}$ is any set of $\text{dim}(\mathcal{X})$ linearly independent elements within $\mathcal{X}$ that also span $\mathcal{X}$.
> <br><br>
> We refer to $\lambda_1, \dots, \lambda_m$ as the **coefficients** of $x$ under the basis $\mathscr{X}$.

Generally speaking, linear spaces can be defined using any object. We will primarily focus on linear spaces of vectors, matrices, and functions. We will go into more detail about each of these objects in following sections. We will see that many of the linear spaces we work with also have additional structure which make them subspaces.
> **Def. Subspace**: A linear space, $\mathcal{V}$, over a field $\mathbb{F}$, is a **subspace** if the following properties are satisfied:
> - **additive closure**: $u + v \in \mathcal{V}, \forall u, v \in \mathcal{V}$, and
> - **multiplicative closure**: $sv \in \mathcal{V}, \forall s \in \mathbb{F}, v \in \mathcal{V}$.

The beauty of a linear space that is also a subspace is that any properties enjoyed by its elements are preserved under the operations of the space. It turns out that both $\mathbb{R}$ and $\mathbb{C}$ are subspaces.
> **Sum of Subspaces**: Let $\mathcal{U}$, and $\mathcal{V}$ be subspaces of $\mathcal{X}$. We define their sum as
> \\[\mathcal{U} + \mathcal{V} := \left\lbrace u + v, u \in \mathcal{U}, v \in \mathcal{V} \right\rbrace.\\]
> If $\mathscr{U}$ is a basis for $\mathcal{U}$ and $\mathscr{V}$ is a basis for $\mathcal{V}$, then $\mathscr{U} \cup \mathscr{V}$ is a basis for $\mathcal{U} + \mathcal{V}$.

> **Def. Independent and Complement Subspaces**: Let $\mathcal{U}$ and $\mathcal{V}$ be subspaces of $\mathcal{X}$. We say that $\mathcal{U}$ and $\mathcal{V}$ are **independent** if $\mathcal{U} \cap \mathcal{V} = \left\lbrace \textbf{0} \right\rbrace$ and **complements** if $\mathcal{U} + \mathcal{V} = \mathcal{X}$.
> <br><br>
> It turns out that every subspace of a linear space has an independent complement.

### Norms and Normed Linear Spaces
> **Def. Norm**: Let $\mathcal{X}$ be a linear space over the field, $\mathbb{F}$. A function, $\|\cdot\|: \mathcal{X} \rightarrow \mathbb{F}$ is a **norm** iff the following properties are satisfied:
> -  **sub-additivity**: $\\|x + y \\| \leq \\|x\\| + \\|y\\|$ for all $x, y \in \mathcal{X}$.
> -  **absolute homogeneity**: $\\|sx\\| = \vert s\vert \\|x\\|$ for all $x \in \mathcal{X}$ and $s \in \mathbb{F}$.
> -  **non-negativity**: $\\|x\\| \geq 0$ for all $x \in \mathcal{X}$ and $\\|x\\| = 0 \Rightarrow x = \mathbf{0}$.

A linear space endowed with a norm is called a normed inner product space. We give a formal definition below.
> **Normed Linear Space**: A **normed linear space** is a linear space, $\mathcal{X}$ endowed with a norm, $\\|\cdot\\|:\mathcal{X} \rightarrow \mathbb{F}$.

### Inner Products and Inner Product Spaces
> **Def. Inner-Product**: Let $\mathcal{X}$ be a linear space over the field $\mathbb{F}$. An **inner-product** is a function, $\langle \cdot, \cdot \rangle: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{F}$ such that:
> -  **linearity**: $\langle sx, y \rangle = s\langle x, y\rangle$ and $\langle x + y, z \rangle = \langle x, y \rangle + \langle y, z \rangle$, for all $x, y, z \in \mathcal{X}$ and $s \in \mathbb{F}$.
> -  **positivity**: $\langle x, x \rangle \geq 0$ if $x \neq \mathbf{0}$.
> -  **conjugate symmetry**: $\langle x, y \rangle = \overline{\langle y, x \rangle}$.
> These properties imply that $\langle x, x \rangle = 0$ if and only if $x = \mathbf{0}$.

> **Def. Orthogonality**: Let $\mathcal{X}$ be an inner-product space. Two elements, $x, y \in \mathcal{X}$ are **orthogonal** iff $\langle x, y \rangle = \mathbf{0}$.

> **Thm. Mutual Orthogonality implies Linear Independence**: Let $\mathcal{X}$ be an inner-product space over a field, $\mathbb{F}$. The elements $x^{(1)}, \dots, x^{(n)} \in \mathcal{X}$ are linearly independent if and only if they are orthogonal

<details>
 <summary><strong>See proof</strong>.</summary>
<p>
Assume $x^{(1)}, \dots, x^{(n)}$ are mutually orthogonal, i.e., $\langle x^{(i)}, x^{(j)} \rangle = \mathbf{0}$ for all $i \neq j$, but that they are not linearly independent, i.e.,
\[\sum_{i=1}^{n}\lambda_ix^{(i)} = \mathbf{0}\]
and $\lambda_k \neq 0$ for some $k$. Taking the inner-product of both sides of the above equation with $x^{(j)}$, we have
\[\left\langle x^{(k)}, \sum_{i=1}^{n}\lambda_ix^{(i)}\right\rangle = \lambda_k\langle x^{(k)}, x^{(k)} \rangle = \langle x^{(k)}, \mathbf{0} \rangle = 0.\]
This is a contradiction since $\langle x^{(k)}, x^{(k)} \rangle \neq 0$.
<br>
$\blacksquare$
</p>
</details>

A linear space endowed with an inner product is called an inner product space. We give a formal definition below.
> **Def. Inner-Product Space**: An **inner product space** is a linear space, $\mathcal{X}$, endowed with an inner-product $\langle \cdot, \cdot \rangle: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{F}$.

It turns out that any inner-product $\langle \cdot, \cdot \rangle$ induces a norm, $\\|\cdot\\|$ through the mapping $\\|x\\| = \langle x, x \rangle$, and thus every inner product space is also a normed space.
> **Thm. Orthogonal Complement**: Let $\mathcal{V}$ be a subspace of an inner-product space $\mathcal{X}$. The **orthogonal complemen**t of $\mathcal{V}$, is
> \\[\mathcal{V}^{\top} := \left\lbrace x \in \mathcal{X} : \langle x, v \rangle = 0, \forall v \in \mathcal{V} \right\rbrace.\\]

<details>
 <summary><strong>See proof</strong>.</summary>
<p>
Clearly $\mathcal{V}$ and $\mathcal{V}^{\top}$ are independent. Indeed, $v \in (\mathcal{V}^{\top} \cap \mathcal{V}) \Leftrightarrow \langle v, v \rangle = 0 \Leftrightarrow v = 0$. $\blacksquare$
</p>
</details>


## Linear Transformations
In this section, we review linear transformations.
> **Def. Linear Transformation**: Let $\mathcal{X}$ and $\mathcal{Y}$ be linear spaces over the field $\mathbb{F}$. A **linear transformation**, $\mathbf{L}: \mathcal{X} \rightarrow \mathcal{Y}$, is a function satisfying
> \\[\mathbf{L}(x + \lambda y) = \mathbf{L}(x) + \lambda L(y),\\]
> for all $x, y \in \mathcal{X}$ and $\lambda \in \mathbb{F}$.

> **Def. Image, Kernel, and Rank** Let $\mathbf{L}: \mathcal{X} \rightarrow \mathcal{Y}$ be a linear transformation. If $\mathcal{V}$ is a subspace of $\mathcal{X}$, then the **image** of $\mathcal{V}$ under $\mathbf{L}$ is
> \\[\mathbf{L}(\mathcal{V})= \left\lbrace y \in \mathcal{V} : (\exists x \in \mathcal{X})y=\mathbf{L}(x)\right\rbrace.\\]
> The image of $\mathcal{X}$ under $\mathbf{L}$, i.e., $\mathbf{L}(\mathcal{X})$, is called the **image** of $\mathbf{L}$, and is also denoted as $\text{img}{\mathbf{L}}$. The **kernel** of $\mathbf{L}$, denoted $\text{ker}{\mathbf{L}}$, is the set of elements, $x \in \mathcal{X}$, such that $\mathbf{L}(x) = \mathbf{0}$, i.e.,
> \\[\text{ker}{\mathbf{L}} = \left\lbrace x \in \mathcal{X} : \mathbf{L}(x) = \mathbf{0} \right\rbrace.\\]
> It turns out that $\mathbf{L}(\mathcal{V})$ is a subspace of $\mathcal{Y}$ and $\text{ker}{\mathbf{L}}$ is a subspace of $\mathcal{X}$. The **rank** of $\mathbf{L}$ is the dimension of its image, i.e., $\text{rank}{\mathbf{L}} = \text{dim}{(\text{img}{\mathbf{L}})}$.

> **Def. Injective, Surjective, and Bijective Transformations**: A linear transformation, $\mathbf{L}: \mathcal{X} \rightarrow \mathcal{Y}$ is **injective** (or one-to-one) if
> \\[\mathbf{L}(x^{(1)}) = \mathbf{L}(x^{(2)}) \Leftrightarrow x^{(1)} = x^{(2)}, \forall x^{(1)}, x^{(2)} \in \mathcal{X},\\]
> **surjective** (or onto) if $\text{img}{\mathbf{L}} = \mathcal{Y}$, and **bijective** if it is both injective and surjective. If there exists some bijective
> transformation between $\mathcal{X}$ and $\mathcal{Y}$, then we say that $\mathcal{X}$ and $\mathcal{Y}$ are isomorphisms.

> **Thm. Injectiveness, Surjectiveness, Invertibility, and Rank**:
> For any linear transformation, $\mathbf{L}: \mathcal{X} \rightarrow \mathcal{Y}$, we have
\\[\text{dim}\left(\text{img}{\mathbf{L}}\right) + \text{dim}\left(\text{ker}{\mathbf{L}}\right) = \text{dim}{\mathcal{X}}.\\] Moreover, $\mathbf{L}$ is
> -  injective if and only if $\text{ker}{\mathbf{L}} = \left\lbrace \mathbf{0} \right\rbrace$, or equivalently, if $\text{dim}\left(\text{ker}{\mathbf{L}}\right) = 0$,
> -  and surjective if and only if $\text{rank}{\mathbf{L}} := \text{dim}\left(\text{img}{\mathbf{L}}\right) = \text{dim}{\mathcal{Y}}$.
> <br>
> It follows that if $\mathbf{L}$ is bijective, then $\text{dim}{\mathcal{X}} = \text{dim}{\mathcal{Y}}$. However, the converse is not necessarily true.

Suppose all elements in $\mathcal{X}$ are expressed using the basis $\mathscr{X} = \left\lbrace x^{(1)}, \dots, x^{(m)} \right\rbrace$ and $x = \sum_{i = 1}^{m}x\_ix^{(i)}, x \in \mathcal{X}$. Our goal is to find an expression for $\mathbf{L}$. To that end, consider applying $\mathbf{L}$ on $x$, i.e., $\mathbf{L}(x)$. We have
\\[\mathbf{L}(x) = \mathbf{L}\left(\sum_{i = 1}^{m}x_ix^{(i)}\right) = \sum_{i = 1}^{m}\mathbf{L}\left(x_ix^{(i)}\right) = \sum_{i = 1}^{m}x_i\mathbf{L}\left(x^{(i)}\right).\\]

In other words, the linear transformation of any element in $\mathcal{X}$ can be expressed as a linear combination of the linear transformations of the basis elements of $\mathcal{X}$. Now suppose that all elements in $\mathcal{Y}$ are expressed using the basis $\mathscr{Y} = \left\lbrace y^{(1)}, \dots, y^{(n)} \right\rbrace$ and that $\mathbf{L}\left(x^{(i)}\right) = \sum_{j=1}^{n}t\_{j,i}y^{(i)}$. It follows that
\\[\mathbf{L}(x) = \sum_{i=1}^{m}x_i\left(\sum_{j=1}^{n}t_{j,i}y^{(j)}\right) = \sum_{j=1}^{n}\left(\sum_{i=1}^{m}x_it_{j,i}\right)y^{(j)}.\\]
Observe that each coefficient of $\mathbf{L}(x)$ under $\mathscr{Y}$ is a linear combination of the coefficients of $x$ under $\mathscr{X}$. We see that to define a linear transformation, $\mathbf{L}: \mathcal{X} \rightarrow \mathcal{Y}$, using the bases $\mathscr{X}$ and $\mathscr{Y}$, we need to find the element in $\mathcal{Y}$ (expressed in $\mathscr{Y}$) that each basis element in $\mathcal{X}$ (expressed using $\mathscr{X}$) is mapped to.
<br><br>
Suppose we wish to represent the same transformation but using different bases. In particular, suppose we wish to use $\mathscr{A}$ as the basis for $\mathcal{X}$ and $\mathscr{B}$ as the basis for $Y$. If $\mathbf{L}\_{\mathscr{X},\mathscr{Y}}$ represents the linear transformation in the original basis and $\mathbf{L}\_{\mathscr{A},\mathscr{B}}$ represents it in the new basis, we have
\\[\mathbf{L}\_{\mathscr{A},\mathscr{B}} = \mathbf{T}\_{\mathscr{Y},\mathscr{B}} \circ \mathbf{L}\_{\mathscr{X},\mathscr{Y}} \circ \mathbf{T}\_{\mathscr{A},\mathscr{X}},\\]
where $\mathbf{T}\_{\mathscr{A},\mathscr{X}}$ is the change of basis transformation from $\mathscr{A}$ to $\mathscr{X}$ and $\mathbf{T}\_{\mathscr{Y},\mathscr{B}}$ is the change of basis transformation from $\mathscr{Y}$ to $\mathscr{B}$. A change-of-basis transformation is always linear.
> **Def. Inverse Transform**:
> Let $\mathbf{L}: \mathcal{X} \rightarrow \mathcal{Y}$ be a bijective linear transformation. The **inverse** of $\mathbf{L}$, denoted $\mathbf{L}^{-1}$, is the unique linear transformation such that
> \\[\mathbf{L}^{-1} \circ \mathbf{L}(x) = x, \forall x \in \mathcal{X} \text{ and } \mathbf{L} \circ \mathbf{L}(y) = y, \forall y \in \mathcal{Y}.\\]
> We note that the function $\mathbf{L}^{-1}: \mathcal{Y} \rightarrow \mathcal{X}$ is well-defined. Indeed, the surjectivity of $\mathbf{L}$ guarantees the existence of an $x$ such that $\mathbf{L}(x) = y$, while its injectivity guarantees its uniqueness.

> **Def. Eigen-values and Eigen-elements**: Let $\mathbf{L}: \mathcal{X} \rightarrow \mathcal{Y}$ be some linear transformation, where $\mathcal{X}$ is a linear space over $\mathbb{F}$. An element, $x \in \mathcal{X}$ is an **eigen-element** of $\mathbf{L}$ if $\mathbf{L}(x) = \lambda x$ for some $\lambda \in \mathbb{F}$. 
> We refer to $\lambda$ as the associated **eigen-value**. In other words, the eigen-value/eigen-element pairs satisfy
> \\[\mathbf{L}(x) - \lambda x = \mathbf{0}.\\]
> For a fixed $\lambda$, we define
> \\[\text{eig}(\mathbf{L}, \lambda) = \left\lbrace x \in \mathcal{X} | \mathbf{L}(x) - \lambda x = \mathbf{0} \right\rbrace\\]
> as the $\lambda$-**eigen-space** of $\mathbf{L}$. Since $\mathbf{L}$ is linear, $x \in \textrm{eig}(\mathbf{L}, \lambda) \Leftrightarrow kx \in \textrm{eig}(\mathbf{L}, \lambda), \forall k \in \mathbf{F}$.
> <br><br>
> The **geometric multiplicity** of $\lambda$ is the dimension of the $\lambda$-eigen-space, i.e., $\dim{\textrm{eig}(\mathbf{L}, \lambda)}$.

## Vectors and Linear Spaces of Vectors
We now consider vectors and linear spaces of vectors.
> **Def. Vector** Let $n$ be a positive integer. An $n$-element column (resp. row) vector, $v$, over a field $\mathbb{F}$ (typically $\mathbb{R}$ or $\mathbb{C}$), is an $n$-tuple of elements in $\mathbb{F}$ stacked in a column (resp. row), i.e.,
> \\[v = \begin{bmatrix} v_1 \\\\ \vdots \\\\ v_n \end{bmatrix} \left(\text{resp. } v = \begin{bmatrix} v_1 & \dots & v_n \end{bmatrix}\right), v_i \in \mathbb{F}.\\]
> We write $v \in \mathbb{F}^{n}$. By convention, we assume (unless explicitly stated) that all vectors are column vectors. The set of all $n$-element (column) vectors over $\mathbb{F}$ is denoted $\mathbb{F}^n$.


> **Def. Vector Transpose** The **transpose** of an $n$-element column (resp. row) vector, $v$, denoted $v^{\top}$, is an $n$-element row (resp. column) vector containing the same elements, i.e.,
> \\[v = \begin{bmatrix} v_1 & \dots & v_n \end{bmatrix} \Leftrightarrow v^{\top} = \begin{bmatrix} v_1 \\\\ \vdots \\\\ v_n \end{bmatrix} \text{ and } v = \begin{bmatrix} v_1 \\\\ \vdots \\\\ v_n \end{bmatrix} \Leftrightarrow v^{\top} = \begin{bmatrix} v_1 & \dots & v_n \end{bmatrix}.\\]

In order for $\mathbb{F}^{n}$ to be a linear space, we need to define the notions of addition between two vectors, and multiplication of a vector with a scalar. These definitions should be consistent with their scalar equivalents when $n = 1$.
> **Def. Vector Addition**: Let $u, v \in \mathbb{F}^{n}$. We define their sum as
> \\[u + v := \begin{bmatrix} u_1 + v_1 \\\\ \vdots \\\\ u_n + v_n \end{bmatrix},\\]
> where $u_i + v_i$ is defined implicitly by $\mathbb{F}$.

> **Def. Vector-Scalar Multiplication**: Let $v \in \mathbb{F}^{n}$ and $s \in \mathbb{F}$. We define their product as
> \\[sv = \begin{bmatrix} sv_1 \\\\ \vdots \\\\ sv_n \end{bmatrix},\\]
> where $sv_i$ is defined implicitly by $\mathbb{F}$.

With these operations, $\mathbb{F}^{n}$ becomes a linear space. This is expected since both operations are defined entirely in terms of their equivalents within the underlying field, which itself is a linear space. Moreover, $\mathbb{R}^{n}$ and $\mathbb{C}^n$ are subspaces of themselves.
<br><br>
All of these spaces have a dimensionality of $n$. So far, we have been representing vectors within them using the so-called \textbf{standard basis} for $\mathbb{F}^{n}$, i.e.,
\\[\left\lbrace \mathbf{e}^{(1)}, \dots, \mathbf{e}^{(n)} \right\rbrace,\\]
where $\mathbf{e}^{(i)} \in \mathbb{F}^{n}$ is the vector with every element being the additive identity of $\mathbb{F}$, except the $i$<sup>th</sup>, which is the multiplicative identity of $\mathbb{F}$. In other words, when we write $\begin{bmatrix}
x_1 & \dots & x_n
\end{bmatrix}^{\top}$, we mean that $x = \sum_{i=1}^{n}x_i\mathbf{e}^{(i)}$. However, it is also possible to represent it using a different basis.
> **Def. Basis Representation**: Let $\mathscr{X} = \lbrace x^{(1)}, \dots, x^{(n)} \rbrace$ be a (possibly non-standard) basis for the linear space $\mathcal{X}$ over the field $\mathbb{F}$. When we write $x = \begin{bmatrix}x_1 & \dots & x_n \end{bmatrix}^{\top}\_{\mathscr{X}}$ we mean
> \\[x = \sum_{i=1}^{n}x_ix^{(i)}.\\]

Let us now define an inner-product for vectors.
> **Def. Vector Inner Product**: We define the inner product of an $n$-element row vector, $v$, and $n$-element column vector $w$, as
> \\[vw = \begin{bmatrix} v_1 & \dots & v_n \end{bmatrix}\begin{bmatrix} w_1 \\\\ \vdots \\\\ w_n \end{bmatrix} = \sum_{i = 1}^{n}v_iw_i.\\]

With this inner-product, $\mathbb{F}^{n}$ becomes an inner-product space. It is possible to define the vector inner-product in other ways. One use of defining the vector inner-product in the way we have is that it allows us to compactly express a linear equation, $\sum_{i=1}^{n}a_ix_i = b$, in vector form as $a^{\top}x = b$, where $a^{\top} = \begin{bmatrix}
a_1 & \dots & a_n
\end{bmatrix}$ and $x^{\top} = \begin{bmatrix}
x_1 & \dots & x_n
\end{bmatrix}$.

## Matrices and Linear Spaces of Matrices
We now consider matrices and linear spaces of matrices.
> **Def. Matrix**: Let $n$ and $m$ be positive integers. An $n \times m$ dimensional matrix, $A$, over a field $\mathbb{F}$ (typically $\mathbb{R}$ or $\mathbb{C}$) is an array of $n$ rows and $m$ columns of elements in $\mathbb{F}$, i.e.,
> \\[A = \begin{bmatrix}a_{1,1} & \dots & a_{n,1} \\\\ \vdots & \ddots & \vdots \\\\ a_{n,1} & \dots & a_{n,m} \end{bmatrix}, a_{i,j} \in \mathbb{F}.\\]
> We write $A \in \mathbb{F}^{n,m}$. If $n = m$ we say that $A$ is a **square matrix**. If $A$ is a square matrix and $a_{i,j} = 0$ whenever $i \neq j$, we say that it is a **diagonal matrix** and often denote it concisely as $\textrm{diag}(a_{1,1}, \dots, a_{n,n})$.

In order for $\mathbb{F}^{n,m}$ to be a linear space, we need to define notions of addition between two matrices, and multiplication of a matrix with a scalar. These definitions should be consistent with their vector equivalents when $m = 1$.
> **Def. Matrix Addition**: Let $A, B \in \mathbb{F}^{n,m}$. We define their sum as
> \\[A + B := \begin{bmatrix} a_{1,1} + b_{1,1} & \dots & a_{1,m} + b_{1,m} \\\\ \vdots & \ddots & \vdots \\\\ a_{n,1} + b_{n,1} & \dots & a_{n,m} + b_{n,m} \end{bmatrix}.\\]
> It is easy to see that if $m = 1$, the operation reduces to that of Def. \ref{}.

> **Def. Matrix-Scalar Multiplication**: Let $A \in \mathbb{F}^{n,m}$ and $s \in \mathbb{F}$. We define their product as 
> \\[sA := \begin{bmatrix} sa_{1,1} & \dots & sa_{1,m} \\\\ \vdots & \ddots & \vdots \\\\ sa_{n,1} & \dots & sa_{n,m} \end{bmatrix}.\\]
> It is easy to see that if $m = 1$, the operation reduces to that of Def. \ref{}.

With these operations, $\mathbb{F}^{n,m}$ becomes a linear space. This is expected since both operations are defined entirely in terms of their equivalents within the underlying field, which itself is a linear space. Moreover, $\mathbb{R}^{n,m}$ and $\mathbb{C}^{n,m}$ are subspaces of themselves.
<br><br>
All of these spaces have a dimensionality of $mn$. So far, we have been representing matrices using the so-called \textbf{standard basis} for $\mathbb{F}^{n,m}$, i.e.,
\\[\left\lbrace \mathbf{e}^{(1,1)}, \dots, \mathbf{e}^{(n,1)}, \mathbf{e}^{(1,2)}, \dots, \mathbf{e}^{(m-1,n)}, \mathbf{e}^{(m,1)} \dots, \mathbf{e}^{(m,n)} \right\rbrace,\\]
where $\mathbf{e}^{(i,j)} \in \mathbb{F}^{n,m}$ is the matrix with every element being the additive identity of $\mathbb{F}$, except for the element in the $i$<sup>th</sup> row and $j$<sup>th</sup> column which is the multiplicative identity of $\mathbb{F}$. In other words, when we write
\\[A = \begin{bmatrix}
a_{1,1} & \dots & a_{m,1} \\\\
\vdots & \ddots & \vdots \\\\
a_{n,1} & \dots & a_{m,n}
\end{bmatrix},\\]
we mean $A = \sum_{i=1}^{n}\sum_{j=1}^{m}a_{i,j}\mathbf{e}^{(i,j)}$. While it is also possible to represent it using a different basis, we will always use the standard basis.

> **Def. Columns and Rows of Matrices**: Let $A \in \mathbb{F}^{n,m}$. The $i$<sup>th</sup> column of $A$ is an $n$-element column vector,
> \\[\begin{bmatrix} a_{1,i} \\\\ \vdots \\\\ a_{n, i} \end{bmatrix},\\]
> and the $j$<sup>th</sup> row of $A$ is an $m$-element row vector,
> \\[\begin{bmatrix} a_{j,1} & \dots & a_{j,m} \end{bmatrix}.\\]

Using this interpretation, we can naturally define the product of two matrices in terms of the product of two vectors.
> **Def. Matrix-Matrix Multiplication**: Let $A \in \mathbb{F}^{n,p}, B \in \mathbb{F}^{p,m}$. We define their product, $AB \in \mathbb{F}^{n,m}$, element-wise so that
> \\[(AB)\_{i,j} = \sum_{k=1}^{m}a_{i,k}b_{k,j}, i = 1, \dots, n, j = 1, \dots, m.\\]
> In other words, the $i$<sup>th</sup> row and $j$<sup>th</sup> column of $AB$ is the vector-vector product of the the $i$<sup>th</sup> row of $A$ and the $j$<sup>th</sup> column of $B$.

Def. \ref{} holds still holds if $A$ is a row vector, i.e., $n = 1$, or $B$ is a column vector, i.e., $p = 1$, and consistent with Def. \ref{}. Furthermore, it allows us to compactly express a system of linear equations,
\\[\begin{aligned}
\sum_{i=1}^{m}a_{1,i}x_i &= b_1 \\\\ \vdots \\\\ \sum_{i=1}^{m}a_{n,i}x_i &= b_n
\end{aligned}\\]
as $Ax = b$, where
\\[A = \begin{bmatrix}
a_{1,1} & \dots & a_{1,m} \\\\ \vdots & \ddots & \vdots \\\\ a_{n,1} & \dots & a_{n,m}
\end{bmatrix}, x = \begin{bmatrix}
x_1 \\\\ \vdots \\\\ x_m
\end{bmatrix} \text{ and } b = \begin{bmatrix}
b_1 \\\\ \vdots \\\\ b_n
\end{bmatrix}.\\]
Moreover, it lets us represent any linear transformation using a matrix. Consider $\mathbf{L}: \mathcal{X} \rightarrow \mathcal{Y}$ using the bases $\mathscr{X}$ and $\mathscr{Y}$. Recall that if $x = \sum_{i=1}^{m}x_ix^{(i)}$, then the $j$<sup>th</sup> coefficient of $\mathbf{L}(x)$ is given by $\sum_{i=1}^{m}x_it_{j,i}$, where $t_{j,i}$ satisfies $\mathbf{L}(x^{(i)}) = \sum_{j=1}^{n}t_{j,i}y^{(i)}$. In matrix form,
\\[\mathbf{L}(x) = \underbrace{\begin{bmatrix} t_{1,1} & \dots & t_{1,m} \\\\ \vdots & \ddots & \vdots \\\\ t_{n,1} & \dots & t_{n,m} \end{bmatrix}}\_{L_{\mathscr{X},\mathscr{Y}}}\begin{bmatrix} x_1 \\\\ \vdots \\\\ x_m \end{bmatrix}.\\]
In other words, $L_{\mathscr{X},\mathscr{Y}}$ encodes the transformation $\mathbf{L}$ under $\mathscr{X}$ and $\mathscr{Y}$. We see that the $i$<sup>th</sup> column of the $T_{\mathscr{X},\mathscr{Y}}$ is the representation of $x^{(i)}$ in $\mathscr{Y}$. However, this representation is not necessarily unique. Indeed, it depends on the choice of basis representing $\mathcal{X}$. If instead we sought the representation of $\mathbf{L}$ under $\mathcal{A}$ and $\mathcal{B}$, we can employ the change-of-bases transformations, i.e.,
\\[L_{\mathscr{A},\mathscr{B}} = T_{\mathscr{Y},\mathscr{B}}L_{\mathscr{X},\mathscr{Y}}T_{\mathscr{A},\mathscr{X}}.\\]
If $\mathbf{L}$ is an isomorphism, and $L_{\mathscr{X},\mathscr{X}}$ is its matrix representation under $\mathscr{X}$, and we seek its representation under $\mathscr{A}$, i.e., $L_{\mathscr{A},\mathscr{A}}$, we have
\\[L_{\mathscr{A},\mathscr{A}} = T_{\mathscr{X},\mathscr{A}}L_{\mathscr{X},\mathscr{X}}T_{\mathscr{A},\mathscr{X}}.\\]

> **Def. Similar Matrices**: A square matrix, $A$, is **similar** to a square matrix $X$, denoted $A \equiv X$, if there exists a matrix $T$, such that $TA = XT$.

As we just saw, similar matrices represent the same transformation under different bases.
> **Def. Diagonalizable Matrix**: A square matrix, $A$, is **diagonalizable** if it is similar to a diagonal matrix, i.e., there exists a diagonal matrix $\Delta$, such that $TA = \Delta T$ for some matrix $T$. 

Diagonal matrices enjoy a wide range of useful properties. Thus, it is often useful to determine when a matrix is diagonalizable and how to actually diagonalize it. To determine this, we will need to define a few additional concepts.
